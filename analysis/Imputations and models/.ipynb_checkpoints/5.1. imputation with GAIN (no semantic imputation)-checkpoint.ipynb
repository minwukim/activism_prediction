{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2acb5f",
   "metadata": {},
   "source": [
    "# Structure\n",
    "\n",
    "Imputation with GAIN\n",
    "https://github.com/dongdongdongdwn/GAIN-Dovey/blob/master/README.md\n",
    "- 1. no column creation\n",
    "    - 1.1. raw data\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "    - 1.2. ADASYN imputation\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "- 2. yes column creation\n",
    "    - 2.1. raw data\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "    - 2.2. ADASYN imputation\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a391119",
   "metadata": {},
   "source": [
    "## Load the data and the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46fdda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.neighbors._base \n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from missforest.missforest import MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ce798c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../database/2016-2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57082e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = ['unequal_voting','classified_board_system','poison_pill','operating_margin_below_3y_average']\n",
    "non_ratio_variables = [\n",
    "    \"capex\",\n",
    "    \"net_capex\",\n",
    "    \"short_term_wc\",\n",
    "    \"long_term_wc\",\n",
    "    \"modified_wc\",\n",
    "    \"ebitda\",\n",
    "    \"ebit\",\n",
    "    \"net_income\",\n",
    "    \"net_debt\",\n",
    "    \"ev\",\n",
    "    \"repurchase\",\n",
    "    \"board_size\",\n",
    "    \"net_repurchase\",\n",
    "    \"total_compensation_to_executives\",\n",
    "    \"total_compensation_to_board_members\",\n",
    "    \"dividend_to_common\",\n",
    "    \"dividend_to_preferred\"\n",
    "]\n",
    "\n",
    "df['ev_ebitda'] = np.where((df['ev'] != 0) & (df['ebitda'] != 0), df['ev'] / df['ebitda'], np.nan)\n",
    "df['ev_ebit'] = np.where((df['ev'] != 0) & (df['ebit'] != 0), df['ev'] / df['ebit'], np.nan)\n",
    "\n",
    "ratio_variables = [\n",
    "    \"ebitda_margin\",\n",
    "    \"operating_margin\",\n",
    "    \"sales_to_total_assets\",\n",
    "    \"roe\",\n",
    "    \"normalized_roe\",\n",
    "    \"operating_roe\",\n",
    "    \"operating_roic\",\n",
    "    \"eps_adjusted_diluted\",\n",
    "    \"ev_to_sales\",\n",
    "    \"tobin_q_ratio\",\n",
    "    \"pb_ratio\",\n",
    "    \"pe_ratio\",\n",
    "    \"fcf_to_equity\",\n",
    "    \"sales_growth_rate\",\n",
    "    \"dividend_per_share\",\n",
    "    \"dividend_payout_ratio\",\n",
    "    \"asset_to_equity\",\n",
    "    \"cash_conversion_cycle\",\n",
    "    \"ev_ebitda\",\n",
    "    \"ev_ebit\",\n",
    "]\n",
    "\n",
    "technical_variables = [\n",
    "    \"free_float_percentage\",\n",
    "    \"rsi_14d\",\n",
    "    \"rsi_30d\",\n",
    "    \"volatility_30d\",\n",
    "    \"volatility_90d\",\n",
    "    \"volatility_180d\",\n",
    "    \"volume_30d_average_to_outstanding\",\n",
    "    \"insider_shares_percentage\",\n",
    "    \"institution_ownership_percentage\",\n",
    "    \"ceo_tenure\",\n",
    "    \"total_return_5y\",\n",
    "    \"total_return_4y\",\n",
    "    \"total_return_3y\",\n",
    "    \"total_return_2y\",\n",
    "    \"total_return_1y\",\n",
    "    \"total_return_6m\",\n",
    "    \"total_return_3m\",\n",
    "    \"employee_growth_rate\",\n",
    "    \"fcf_yield\"\n",
    "]\n",
    "\n",
    "supportive = [\"bic_level_2\",\"bic_level_3\",\"market_cap\"]\n",
    "factors = binary + non_ratio_variables + ratio_variables + technical_variables\n",
    "\n",
    "df[\"bic_level_2\"] = df[\"bic_level_2\"].astype('category')\n",
    "df[\"bic_level_3\"] = df[\"bic_level_3\"].astype('category')\n",
    "\n",
    "# factors.append(\"targeted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897f954",
   "metadata": {},
   "source": [
    "## 1. No column creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1007d1",
   "metadata": {},
   "source": [
    "### 1.1. raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f913eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary packages\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "##IF USING TF 2 use following import to still use TF < 2.0 Functionalities\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "def normalization (data, parameters=None):\n",
    "  '''Normalize data in [0, 1] range.\n",
    "  \n",
    "  Args:\n",
    "    - data: original data\n",
    "  \n",
    "  Returns:\n",
    "    - norm_data: normalized data\n",
    "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
    "  '''\n",
    "\n",
    "  # Parameters\n",
    "  _, dim = data.shape\n",
    "  norm_data = data.copy()\n",
    "  \n",
    "  if parameters is None:\n",
    "  \n",
    "    # MixMax normalization\n",
    "    min_val = np.zeros(dim)\n",
    "    max_val = np.zeros(dim)\n",
    "    \n",
    "    # For each dimension\n",
    "    for i in range(dim):\n",
    "      min_val[i] = np.nanmin(norm_data[:,i])\n",
    "      norm_data[:,i] = norm_data[:,i] - np.nanmin(norm_data[:,i])\n",
    "      max_val[i] = np.nanmax(norm_data[:,i])\n",
    "      norm_data[:,i] = norm_data[:,i] / (np.nanmax(norm_data[:,i]) + 1e-6)   \n",
    "      \n",
    "    # Return norm_parameters for renormalization\n",
    "    norm_parameters = {'min_val': min_val,\n",
    "                       'max_val': max_val}\n",
    "\n",
    "  else:\n",
    "    min_val = parameters['min_val']\n",
    "    max_val = parameters['max_val']\n",
    "    \n",
    "    # For each dimension\n",
    "    for i in range(dim):\n",
    "      norm_data[:,i] = norm_data[:,i] - min_val[i]\n",
    "      norm_data[:,i] = norm_data[:,i] / (max_val[i] + 1e-6)  \n",
    "      \n",
    "    norm_parameters = parameters    \n",
    "      \n",
    "  return norm_data, norm_parameters\n",
    "\n",
    "\n",
    "def renormalization (norm_data, norm_parameters):\n",
    "  '''Renormalize data from [0, 1] range to the original range.\n",
    "  \n",
    "  Args:\n",
    "    - norm_data: normalized data\n",
    "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
    "  \n",
    "  Returns:\n",
    "    - renorm_data: renormalized original data\n",
    "  '''\n",
    "  \n",
    "  min_val = norm_parameters['min_val']\n",
    "  max_val = norm_parameters['max_val']\n",
    "\n",
    "  _, dim = norm_data.shape\n",
    "  renorm_data = norm_data.copy()\n",
    "    \n",
    "  for i in range(dim):\n",
    "    renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n",
    "    renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n",
    "    \n",
    "  return renorm_data\n",
    "\n",
    "\n",
    "def rounding (imputed_data, data_x):\n",
    "  '''Round imputed data for categorical variables.\n",
    "  \n",
    "  Args:\n",
    "    - imputed_data: imputed data\n",
    "    - data_x: original data with missing values\n",
    "    \n",
    "  Returns:\n",
    "    - rounded_data: rounded imputed data\n",
    "  '''\n",
    "  \n",
    "  _, dim = data_x.shape\n",
    "  rounded_data = imputed_data.copy()\n",
    "  \n",
    "  for i in range(dim):\n",
    "    temp = data_x[~np.isnan(data_x[:, i]), i]\n",
    "    # Only for the categorical variable\n",
    "    if len(np.unique(temp)) < 20:\n",
    "      rounded_data[:, i] = np.round(rounded_data[:, i])\n",
    "      \n",
    "  return rounded_data\n",
    "\n",
    "\n",
    "def rmse_loss (ori_data, imputed_data, data_m):\n",
    "  '''Compute RMSE loss between ori_data and imputed_data\n",
    "  \n",
    "  Args:\n",
    "    - ori_data: original data without missing values\n",
    "    - imputed_data: imputed data\n",
    "    - data_m: indicator matrix for missingness\n",
    "    \n",
    "  Returns:\n",
    "    - rmse: Root Mean Squared Error\n",
    "  '''\n",
    "  \n",
    "  ori_data, norm_parameters = normalization(ori_data)\n",
    "  imputed_data, _ = normalization(imputed_data, norm_parameters)\n",
    "    \n",
    "  # Only for missing values\n",
    "  nominator = np.sum(((1-data_m) * ori_data - (1-data_m) * imputed_data)**2)\n",
    "  denominator = np.sum(1-data_m)\n",
    "  \n",
    "  rmse = np.sqrt(nominator/float(denominator))\n",
    "  \n",
    "  return rmse\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "  '''Xavier initialization.\n",
    "  \n",
    "  Args:\n",
    "    - size: vector size\n",
    "    \n",
    "  Returns:\n",
    "    - initialized random vector.\n",
    "  '''\n",
    "  in_dim = size[0]\n",
    "  xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "  return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "      \n",
    "\n",
    "def binary_sampler(p, rows, cols):\n",
    "  '''Sample binary random variables.\n",
    "  \n",
    "  Args:\n",
    "    - p: probability of 1\n",
    "    - rows: the number of rows\n",
    "    - cols: the number of columns\n",
    "    \n",
    "  Returns:\n",
    "    - binary_random_matrix: generated binary random matrix.\n",
    "  '''\n",
    "  unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
    "  binary_random_matrix = 1*(unif_random_matrix < p)\n",
    "  return binary_random_matrix\n",
    "\n",
    "\n",
    "def uniform_sampler(low, high, rows, cols):\n",
    "  '''Sample uniform random variables.\n",
    "  \n",
    "  Args:\n",
    "    - low: low limit\n",
    "    - high: high limit\n",
    "    - rows: the number of rows\n",
    "    - cols: the number of columns\n",
    "    \n",
    "  Returns:\n",
    "    - uniform_random_matrix: generated uniform random matrix.\n",
    "  '''\n",
    "  return np.random.uniform(low, high, size = [rows, cols])       \n",
    "\n",
    "\n",
    "def sample_batch_index(total, batch_size):\n",
    "  '''Sample index of the mini-batch.\n",
    "  \n",
    "  Args:\n",
    "    - total: total number of samples\n",
    "    - batch_size: batch size\n",
    "    \n",
    "  Returns:\n",
    "    - batch_idx: batch index\n",
    "  '''\n",
    "  total_idx = np.random.permutation(total)\n",
    "  batch_idx = total_idx[:batch_size]\n",
    "  return batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a858e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gain (data_x, gain_parameters):\n",
    "\n",
    "  # Define mask matrix\n",
    "  data_m = 1-np.isnan(data_x)\n",
    "  \n",
    "  # System parameters\n",
    "  batch_size = gain_parameters['batch_size']\n",
    "  hint_rate = gain_parameters['hint_rate']\n",
    "  alpha = gain_parameters['alpha']\n",
    "  iterations = gain_parameters['iterations']\n",
    "  \n",
    "  # Other parameters\n",
    "  no, dim = data_x.shape\n",
    "  \n",
    "  # Hidden state dimensions\n",
    "  h_dim = int(dim)\n",
    "  \n",
    "  # Normalization\n",
    "  norm_data, norm_parameters = normalization(data_x)\n",
    "  norm_data_x = np.nan_to_num(norm_data, 0)\n",
    "  \n",
    "  ## GAIN architecture   \n",
    "  # Input placeholders\n",
    "  # Data vector\n",
    "  X = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  # Mask vector \n",
    "  M = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  # Hint vector\n",
    "  H = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  \n",
    "  # Discriminator variables\n",
    "  D_W1 = tf.Variable(xavier_init([dim*2, h_dim])) # Data + Hint as inputs\n",
    "  D_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  D_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "  D_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  D_W3 = tf.Variable(xavier_init([h_dim, dim]))\n",
    "  D_b3 = tf.Variable(tf.zeros(shape = [dim]))  # Multi-variate outputs\n",
    "  \n",
    "  theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "  \n",
    "  #Generator variables\n",
    "  # Data + Mask as inputs (Random noise is in missing components)\n",
    "  G_W1 = tf.Variable(xavier_init([dim*2, h_dim]))  \n",
    "  G_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  G_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "  G_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  G_W3 = tf.Variable(xavier_init([h_dim, dim]))\n",
    "  G_b3 = tf.Variable(tf.zeros(shape = [dim]))\n",
    "  \n",
    "  theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "  \n",
    "  ## GAIN functions\n",
    "  # Generator\n",
    "  def generator(x,m):\n",
    "    # Concatenate Mask and Data\n",
    "    inputs = tf.concat(values = [x, m], axis = 1) \n",
    "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)   \n",
    "    # MinMax normalized output\n",
    "    G_prob = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3) \n",
    "    return G_prob\n",
    "      \n",
    "  # Discriminator\n",
    "  def discriminator(x, h):\n",
    "    # Concatenate Data and Hint\n",
    "    inputs = tf.concat(values = [x, h], axis = 1) \n",
    "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob\n",
    "  \n",
    "  ## GAIN structure\n",
    "  # Generator\n",
    "  G_sample = generator(X, M)\n",
    " \n",
    "  # Combine with observed data\n",
    "  Hat_X = X * M + G_sample * (1-M)\n",
    "  \n",
    "  # Discriminator\n",
    "  D_prob = discriminator(Hat_X, H)\n",
    "  \n",
    "  ## GAIN loss\n",
    "  D_loss_temp = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) \\\n",
    "                                + (1-M) * tf.log(1. - D_prob + 1e-8)) \n",
    "  \n",
    "  G_loss_temp = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\n",
    "  \n",
    "  MSE_loss = \\\n",
    "  tf.reduce_mean((M * X - M * G_sample)**2) / tf.reduce_mean(M)\n",
    "  \n",
    "  D_loss = D_loss_temp\n",
    "  G_loss = G_loss_temp + alpha * MSE_loss \n",
    "  \n",
    "  ## GAIN solver\n",
    "  D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "  G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "  \n",
    "  ## Iterations\n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "  # Start Iterations\n",
    "  for it in tqdm(range(iterations)):    \n",
    "      \n",
    "    # Sample batch\n",
    "    batch_idx = sample_batch_index(no, batch_size)\n",
    "    X_mb = norm_data_x[batch_idx, :]  \n",
    "    M_mb = data_m[batch_idx, :]  \n",
    "    # Sample random vectors  \n",
    "    Z_mb = uniform_sampler(0, 0.01, batch_size, dim) \n",
    "    # Sample hint vectors\n",
    "    H_mb_temp = binary_sampler(hint_rate, batch_size, dim)\n",
    "    H_mb = M_mb * H_mb_temp\n",
    "      \n",
    "    # Combine random vectors with observed vectors\n",
    "    X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
    "      \n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss_temp], \n",
    "                              feed_dict = {M: M_mb, X: X_mb, H: H_mb})\n",
    "    _, G_loss_curr, MSE_loss_curr = \\\n",
    "    sess.run([G_solver, G_loss_temp, MSE_loss],\n",
    "             feed_dict = {X: X_mb, M: M_mb, H: H_mb})\n",
    "            \n",
    "  ## Return imputed data      \n",
    "  Z_mb = uniform_sampler(0, 0.01, no, dim) \n",
    "  M_mb = data_m\n",
    "  X_mb = norm_data_x          \n",
    "  X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
    "      \n",
    "  imputed_data = sess.run([G_sample], feed_dict = {X: X_mb, M: M_mb})[0]\n",
    "  \n",
    "  imputed_data = data_m * norm_data_x + (1-data_m) * imputed_data\n",
    "  \n",
    "  # Renormalization\n",
    "  imputed_data = renormalization(imputed_data, norm_parameters)  \n",
    "  \n",
    "  # Rounding\n",
    "  imputed_data = rounding(imputed_data, data_x)  \n",
    "          \n",
    "  return imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b90cead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 15:31:21.171254: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
      "100%|████████████████████████████████████| 10000/10000 [00:14<00:00, 671.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Preparation: Assuming you have already imported pandas as pd and other necessary libraries\n",
    "\n",
    "# 2. Format your data\n",
    "data_x = df[factors].values\n",
    "\n",
    "# 3. Setup GAIN Parameters\n",
    "gain_parameters = {\n",
    "    'batch_size': 128,\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 10,\n",
    "    'iterations': 10000\n",
    "}\n",
    "\n",
    "# 4. Run GAIN\n",
    "imputed_data = gain(data_x, gain_parameters)\n",
    "\n",
    "# 5. Replace Missing Data in DataFrame\n",
    "df[factors] = np.where(np.isnan(df[factors]), imputed_data, df[factors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7db64b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 15:32:37.670673: W tensorflow/c/c_api.cc:305] Operation '{name:'training/Adam/decay/Assign' id:1133 op device:{requested: '', assigned: ''} def:{{{node training/Adam/decay/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/decay, training/Adam/decay/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-10-25 15:32:37.845721: W tensorflow/c/c_api.cc:305] Operation '{name:'loss/mul' id:935 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/dense_40_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.517378  0.516439\n",
      "1        RF   0.612245  0.631038\n",
      "2       XGB   0.638570  0.641958\n",
      "3      LGBM   0.668172  0.658351\n",
      "4  CatBoost   0.659091  0.635290\n",
      "5        NN   0.915842  0.608056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-10-25 15:32:50.999310: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_40/Sigmoid' id:907 op device:{requested: '', assigned: ''} def:{{{node dense_40/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_40/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result1 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfc07a",
   "metadata": {},
   "source": [
    "### 1.2. oversampling with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "352f181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Apply ADASYN oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9241c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 15:34:47.396399: W tensorflow/c/c_api.cc:305] Operation '{name:'training_2/Adam/dense_41/bias/v/Assign' id:1647 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/dense_41/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/dense_41/bias/v, training_2/Adam/dense_41/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-10-25 15:34:47.687624: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_1/mul' id:1398 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/dense_43_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.566993  0.560878\n",
      "1        RF   0.999660  0.645448\n",
      "2       XGB   0.997908  0.607669\n",
      "3      LGBM   0.996437  0.622092\n",
      "4  CatBoost   0.995245  0.603411\n",
      "5        NN   0.998222  0.552332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-10-25 15:35:13.040088: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_43/Sigmoid' id:1370 op device:{requested: '', assigned: ''} def:{{{node dense_43/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_43/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result2 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18d764",
   "metadata": {},
   "source": [
    "## 2.1 with column creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "506b9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../database/2016-2022.csv')\n",
    "df['ev_ebitda'] = np.where((df['ev'] != 0) & (df['ebitda'] != 0), df['ev'] / df['ebitda'], np.nan)\n",
    "df['ev_ebit'] = np.where((df['ev'] != 0) & (df['ebit'] != 0), df['ev'] / df['ebit'], np.nan)\n",
    "df[\"bic_level_2\"] = df[\"bic_level_2\"].astype('category')\n",
    "df[\"bic_level_3\"] = df[\"bic_level_3\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6775b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in non_ratio_variables:\n",
    "    \n",
    "    # 1. _percentile\n",
    "    percentile_col = col + '_percentile'\n",
    "    df[percentile_col] = df.groupby('year')[col].transform(lambda x: x.rank(pct=True) * 100)\n",
    "    df[percentile_col].fillna(50, inplace=True)\n",
    "    \n",
    "    # 2. _10bins_percentile\n",
    "    df['market_cap_bins'] = df.groupby('year')['market_cap'].transform(lambda x: pd.cut(x, bins=10))\n",
    "    percentile_10bins_col = col + '_10bins_percentile'\n",
    "    df[percentile_10bins_col] = df.groupby(['year', 'market_cap_bins'])[col].transform(lambda x: x.rank(pct=True) * 100)\n",
    "    df[percentile_10bins_col].fillna(50, inplace=True)\n",
    "    df.drop('market_cap_bins', axis=1, inplace=True)\n",
    "\n",
    "    # 3. _10bins_normalized\n",
    "    df['market_cap_bins'] = df.groupby('year')['market_cap'].transform(lambda x: pd.qcut(x, 10, labels=False, duplicates='drop'))\n",
    "    normalized_col = col + '_10bins_normalized'\n",
    "    df[normalized_col] = df.groupby(['year', 'market_cap_bins'])[col].transform(lambda x: (x - x.mean()) / x.std())\n",
    "    df[normalized_col].fillna(0, inplace=True)\n",
    "    df.drop('market_cap_bins', axis=1, inplace=True)\n",
    "    \n",
    "    # 4. _div_market_cap\n",
    "    div_market_cap_col = col + '_div_market_cap'\n",
    "    df[div_market_cap_col] = df[col] / df['market_cap']\n",
    "    \n",
    "    # 5. _div_log_market_cap\n",
    "    df['log_market_cap'] = np.log(df['market_cap'])\n",
    "    div_log_market_cap_col = col + '_div_log_market_cap'\n",
    "    df[div_log_market_cap_col] = df[col] / df['log_market_cap']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1626788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentile(group):\n",
    "    if len(group) < 10:\n",
    "        return pd.Series([None] * len(group), index=group.index, dtype=float)\n",
    "    return group.rank(pct=True) * 100\n",
    "\n",
    "def normalize(group):\n",
    "    if len(group) < 10:\n",
    "        return pd.Series([None] * len(group), index=group.index, dtype=float)\n",
    "    return (group - group.mean()) / group.std()\n",
    "\n",
    "for col in ratio_variables:\n",
    "    percentile_col = col + '_industry_peers_percentile'\n",
    "    df[percentile_col] = df.groupby(['year', 'bic_level_3'])[col].transform(compute_percentile)\n",
    "    mask = df[percentile_col].isna()\n",
    "    df.loc[mask, percentile_col] = df[mask].groupby(['year', 'bic_level_2'])[col].transform(compute_percentile)\n",
    "    df[percentile_col].fillna(50, inplace=True)\n",
    "    df[percentile_col] = df[percentile_col].astype(float)\n",
    "    normalized_col = col + '_industry_peers_normalized'\n",
    "    df[normalized_col] = df.groupby(['year', 'bic_level_3'])[col].transform(normalize)\n",
    "    mask = df[normalized_col].isna()\n",
    "    df.loc[mask, normalized_col] = df[mask].groupby(['year', 'bic_level_2'])[col].transform(normalize)\n",
    "    df[normalized_col].fillna(0, inplace=True)\n",
    "    df[normalized_col] = df[normalized_col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "900daeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = []\n",
    "for col in non_ratio_variables:\n",
    "    factors.extend([\n",
    "        col,\n",
    "        f'{col}_percentile',\n",
    "        f'{col}_10bins_percentile',\n",
    "        f'{col}_10bins_normalized',\n",
    "        f'{col}_div_market_cap',\n",
    "        f'{col}_div_log_market_cap'\n",
    "    ])\n",
    "\n",
    "for col in ratio_variables:\n",
    "    factors.extend([\n",
    "        col,\n",
    "        f'{col}_industry_peers_percentile',\n",
    "        f'{col}_industry_peers_normalized'\n",
    "    ])\n",
    "\n",
    "factors = factors + binary + technical_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b385d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10000/10000 [00:46<00:00, 212.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Preparation: Assuming you have already imported pandas as pd and other necessary libraries\n",
    "\n",
    "# 2. Format your data\n",
    "data_x = df[factors].values\n",
    "\n",
    "# 3. Setup GAIN Parameters\n",
    "gain_parameters = {\n",
    "    'batch_size': 128,\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 10,\n",
    "    'iterations': 10000\n",
    "}\n",
    "\n",
    "# 4. Run GAIN\n",
    "imputed_data = gain(data_x, gain_parameters)\n",
    "\n",
    "# 5. Replace Missing Data in DataFrame\n",
    "df[factors] = np.where(np.isnan(df[factors]), imputed_data, df[factors])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63879812",
   "metadata": {},
   "source": [
    "### 2.1. raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8466b0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-10-25 15:39:20.641750: W tensorflow/c/c_api.cc:305] Operation '{name:'training_2/Adam/dense_43/bias/v/Assign' id:1669 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/dense_43/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/dense_43/bias/v, training_2/Adam/dense_43/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-10-25 15:39:20.674122: W tensorflow/c/c_api.cc:305] Operation '{name:'training_4/Adam/dense_46/bias/m/Assign' id:2906 op device:{requested: '', assigned: ''} def:{{{node training_4/Adam/dense_46/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_4/Adam/dense_46/bias/m, training_4/Adam/dense_46/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-10-25 15:39:20.881098: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_2/mul' id:2669 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/dense_46_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.557112  0.551380\n",
      "1        RF   0.602393  0.564660\n",
      "2       XGB   0.644680  0.626791\n",
      "3      LGBM   0.666368  0.638430\n",
      "4  CatBoost   0.662327  0.657538\n",
      "5        NN   0.942548  0.628492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-10-25 15:39:36.221887: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_46/Sigmoid' id:2641 op device:{requested: '', assigned: ''} def:{{{node dense_46/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_46/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result3 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c2684",
   "metadata": {},
   "source": [
    "### 2.2. oversampling the ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21a233d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 15:45:52.403835: W tensorflow/c/c_api.cc:305] Operation '{name:'training_6/Adam/dense_47/bias/m/Assign' id:3347 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/dense_47/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/dense_47/bias/m, training_6/Adam/dense_47/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-10-25 15:45:52.763204: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_3/mul' id:3132 op device:{requested: '', assigned: ''} def:{{{node loss_3/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul/x, loss_3/dense_49_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.644877  0.574362\n",
      "1        RF   0.999942  0.648279\n",
      "2       XGB   0.998472  0.645321\n",
      "3      LGBM   0.997082  0.653208\n",
      "4  CatBoost   0.994339  0.591252\n",
      "5        NN   0.999982  0.622176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-10-25 15:46:23.687476: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_49/Sigmoid' id:3104 op device:{requested: '', assigned: ''} def:{{{node dense_49/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_49/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Apply ADASYN oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result4 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
