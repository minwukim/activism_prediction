{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2acb5f",
   "metadata": {},
   "source": [
    "# Structure\n",
    "\n",
    "Imputation with kNN\n",
    "- 1. no column creation\n",
    "    - 1.1. raw data\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "    - 1.2. ADASYN imputation\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "- 2. yes column creation\n",
    "    - 2.1. raw data\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "    - 2.2. ADASYN imputation\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a391119",
   "metadata": {},
   "source": [
    "## Load the data and the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46fdda19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Oct 24 05:00:49 PM: Encountered unexpected exception importing solver CVXOPT:\n",
      "ImportError(\"dlopen(/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/liblapack.3.dylib\\n  Referenced from: <E25E40AB-7857-39B9-8DE7-28B7B0E4806B> /Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so\\n  Reason: tried: '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/usr/local/lib/liblapack.3.dylib' (no such file), '/usr/lib/liblapack.3.dylib' (no such file, not in dyld cache)\")\n",
      "(CVXPY) Oct 24 05:00:49 PM: Encountered unexpected exception importing solver GLPK:\n",
      "ImportError(\"dlopen(/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/liblapack.3.dylib\\n  Referenced from: <E25E40AB-7857-39B9-8DE7-28B7B0E4806B> /Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so\\n  Reason: tried: '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/usr/local/lib/liblapack.3.dylib' (no such file), '/usr/lib/liblapack.3.dylib' (no such file, not in dyld cache)\")\n",
      "(CVXPY) Oct 24 05:00:49 PM: Encountered unexpected exception importing solver GLPK_MI:\n",
      "ImportError(\"dlopen(/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/liblapack.3.dylib\\n  Referenced from: <E25E40AB-7857-39B9-8DE7-28B7B0E4806B> /Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so\\n  Reason: tried: '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/usr/local/lib/liblapack.3.dylib' (no such file), '/usr/lib/liblapack.3.dylib' (no such file, not in dyld cache)\")\n",
      "(CVXPY) Oct 24 05:00:49 PM: Encountered unexpected exception importing solver SCS:\n",
      "ImportError(\"dlopen(/Users/minwukim/anaconda3/lib/python3.11/site-packages/_scs_direct.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/liblapack.3.dylib\\n  Referenced from: <A08A2CF9-B9A1-393C-A32E-68987F02C61E> /Users/minwukim/anaconda3/lib/python3.11/site-packages/_scs_direct.cpython-311-darwin.so\\n  Reason: tried: '/Users/minwukim/anaconda3/lib/python3.11/site-packages/../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/lib/python3.11/site-packages/../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/usr/local/lib/liblapack.3.dylib' (no such file), '/usr/lib/liblapack.3.dylib' (no such file, not in dyld cache)\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from fancyimpute import KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce798c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../database/2016-2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57082e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = ['unequal_voting','classified_board_system','poison_pill','operating_margin_below_3y_average']\n",
    "non_ratio_variables = [\n",
    "    \"capex\",\n",
    "    \"net_capex\",\n",
    "    \"short_term_wc\",\n",
    "    \"long_term_wc\",\n",
    "    \"modified_wc\",\n",
    "    \"ebitda\",\n",
    "    \"ebit\",\n",
    "    \"net_income\",\n",
    "    \"net_debt\",\n",
    "    \"ev\",\n",
    "    \"repurchase\",\n",
    "    \"board_size\",\n",
    "    \"net_repurchase\",\n",
    "    \"total_compensation_to_executives\",\n",
    "    \"total_compensation_to_board_members\",\n",
    "    \"dividend_to_common\",\n",
    "    \"dividend_to_preferred\"\n",
    "]\n",
    "\n",
    "df['ev_ebitda'] = np.where((df['ev'] != 0) & (df['ebitda'] != 0), df['ev'] / df['ebitda'], np.nan)\n",
    "df['ev_ebit'] = np.where((df['ev'] != 0) & (df['ebit'] != 0), df['ev'] / df['ebit'], np.nan)\n",
    "\n",
    "ratio_variables = [\n",
    "    \"ebitda_margin\",\n",
    "    \"operating_margin\",\n",
    "    \"sales_to_total_assets\",\n",
    "    \"roe\",\n",
    "    \"normalized_roe\",\n",
    "    \"operating_roe\",\n",
    "    \"operating_roic\",\n",
    "    \"eps_adjusted_diluted\",\n",
    "    \"ev_to_sales\",\n",
    "    \"tobin_q_ratio\",\n",
    "    \"pb_ratio\",\n",
    "    \"pe_ratio\",\n",
    "    \"fcf_to_equity\",\n",
    "    \"sales_growth_rate\",\n",
    "    \"dividend_per_share\",\n",
    "    \"dividend_payout_ratio\",\n",
    "    \"asset_to_equity\",\n",
    "    \"cash_conversion_cycle\",\n",
    "    \"ev_ebitda\",\n",
    "    \"ev_ebit\",\n",
    "]\n",
    "\n",
    "technical_variables = [\n",
    "    \"free_float_percentage\",\n",
    "    \"rsi_14d\",\n",
    "    \"rsi_30d\",\n",
    "    \"volatility_30d\",\n",
    "    \"volatility_90d\",\n",
    "    \"volatility_180d\",\n",
    "    \"volume_30d_average_to_outstanding\",\n",
    "    \"insider_shares_percentage\",\n",
    "    \"institution_ownership_percentage\",\n",
    "    \"ceo_tenure\",\n",
    "    \"total_return_5y\",\n",
    "    \"total_return_4y\",\n",
    "    \"total_return_3y\",\n",
    "    \"total_return_2y\",\n",
    "    \"total_return_1y\",\n",
    "    \"total_return_6m\",\n",
    "    \"total_return_3m\",\n",
    "    \"employee_growth_rate\",\n",
    "    \"fcf_yield\"\n",
    "]\n",
    "\n",
    "supportive = [\"bic_level_2\",\"bic_level_3\",\"market_cap\"]\n",
    "factors = binary + non_ratio_variables + ratio_variables + technical_variables\n",
    "\n",
    "df[\"bic_level_2\"] = df[\"bic_level_2\"].astype('category')\n",
    "df[\"bic_level_3\"] = df[\"bic_level_3\"].astype('category')\n",
    "\n",
    "# factors.append(\"targeted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897f954",
   "metadata": {},
   "source": [
    "## 1. No column creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1007d1",
   "metadata": {},
   "source": [
    "### 1.1. raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66a501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/18213 with 5 missing, elapsed time: 73.891\n",
      "Imputing row 101/18213 with 2 missing, elapsed time: 73.965\n",
      "Imputing row 201/18213 with 3 missing, elapsed time: 74.009\n",
      "Imputing row 301/18213 with 4 missing, elapsed time: 74.045\n",
      "Imputing row 401/18213 with 1 missing, elapsed time: 74.082\n",
      "Imputing row 501/18213 with 0 missing, elapsed time: 74.140\n",
      "Imputing row 601/18213 with 1 missing, elapsed time: 74.193\n",
      "Imputing row 701/18213 with 6 missing, elapsed time: 74.231\n",
      "Imputing row 801/18213 with 4 missing, elapsed time: 74.267\n",
      "Imputing row 901/18213 with 1 missing, elapsed time: 74.306\n",
      "Imputing row 1001/18213 with 3 missing, elapsed time: 74.355\n",
      "Imputing row 1101/18213 with 2 missing, elapsed time: 74.404\n",
      "Imputing row 1201/18213 with 3 missing, elapsed time: 74.449\n",
      "Imputing row 1301/18213 with 6 missing, elapsed time: 74.512\n",
      "Imputing row 1401/18213 with 0 missing, elapsed time: 74.549\n",
      "Imputing row 1501/18213 with 15 missing, elapsed time: 74.584\n",
      "Imputing row 1601/18213 with 6 missing, elapsed time: 74.626\n",
      "Imputing row 1701/18213 with 1 missing, elapsed time: 74.661\n",
      "Imputing row 1801/18213 with 17 missing, elapsed time: 74.706\n",
      "Imputing row 1901/18213 with 1 missing, elapsed time: 74.749\n",
      "Imputing row 2001/18213 with 19 missing, elapsed time: 74.793\n",
      "Imputing row 2101/18213 with 6 missing, elapsed time: 74.837\n",
      "Imputing row 2201/18213 with 12 missing, elapsed time: 74.878\n",
      "Imputing row 2301/18213 with 2 missing, elapsed time: 74.922\n",
      "Imputing row 2401/18213 with 6 missing, elapsed time: 74.961\n",
      "Imputing row 2501/18213 with 2 missing, elapsed time: 75.006\n",
      "Imputing row 2601/18213 with 7 missing, elapsed time: 75.046\n",
      "Imputing row 2701/18213 with 9 missing, elapsed time: 75.084\n",
      "Imputing row 2801/18213 with 2 missing, elapsed time: 75.153\n",
      "Imputing row 2901/18213 with 1 missing, elapsed time: 75.192\n",
      "Imputing row 3001/18213 with 12 missing, elapsed time: 75.231\n",
      "Imputing row 3101/18213 with 8 missing, elapsed time: 75.270\n",
      "Imputing row 3201/18213 with 2 missing, elapsed time: 75.316\n",
      "Imputing row 3301/18213 with 4 missing, elapsed time: 75.356\n",
      "Imputing row 3401/18213 with 10 missing, elapsed time: 75.393\n",
      "Imputing row 3501/18213 with 16 missing, elapsed time: 75.432\n",
      "Imputing row 3601/18213 with 2 missing, elapsed time: 75.471\n",
      "Imputing row 3701/18213 with 5 missing, elapsed time: 75.526\n",
      "Imputing row 3801/18213 with 5 missing, elapsed time: 75.566\n",
      "Imputing row 3901/18213 with 6 missing, elapsed time: 75.610\n",
      "Imputing row 4001/18213 with 8 missing, elapsed time: 75.650\n",
      "Imputing row 4101/18213 with 11 missing, elapsed time: 75.685\n",
      "Imputing row 4201/18213 with 12 missing, elapsed time: 75.718\n",
      "Imputing row 4301/18213 with 0 missing, elapsed time: 75.753\n",
      "Imputing row 4401/18213 with 4 missing, elapsed time: 75.795\n",
      "Imputing row 4501/18213 with 11 missing, elapsed time: 75.835\n",
      "Imputing row 4601/18213 with 1 missing, elapsed time: 75.880\n",
      "Imputing row 4701/18213 with 3 missing, elapsed time: 75.926\n",
      "Imputing row 4801/18213 with 12 missing, elapsed time: 75.960\n",
      "Imputing row 4901/18213 with 18 missing, elapsed time: 76.001\n",
      "Imputing row 5001/18213 with 2 missing, elapsed time: 76.044\n",
      "Imputing row 5101/18213 with 3 missing, elapsed time: 76.081\n",
      "Imputing row 5201/18213 with 6 missing, elapsed time: 76.122\n",
      "Imputing row 5301/18213 with 12 missing, elapsed time: 76.163\n",
      "Imputing row 5401/18213 with 42 missing, elapsed time: 76.217\n",
      "Imputing row 5501/18213 with 1 missing, elapsed time: 76.282\n",
      "Imputing row 5601/18213 with 21 missing, elapsed time: 76.335\n",
      "Imputing row 5701/18213 with 1 missing, elapsed time: 76.380\n",
      "Imputing row 5801/18213 with 5 missing, elapsed time: 76.438\n",
      "Imputing row 5901/18213 with 1 missing, elapsed time: 76.494\n",
      "Imputing row 6001/18213 with 6 missing, elapsed time: 76.569\n",
      "Imputing row 6101/18213 with 5 missing, elapsed time: 76.684\n",
      "Imputing row 6201/18213 with 2 missing, elapsed time: 76.734\n",
      "Imputing row 6301/18213 with 9 missing, elapsed time: 76.793\n",
      "Imputing row 6401/18213 with 14 missing, elapsed time: 76.847\n",
      "Imputing row 6501/18213 with 9 missing, elapsed time: 76.889\n",
      "Imputing row 6601/18213 with 6 missing, elapsed time: 76.931\n",
      "Imputing row 6701/18213 with 0 missing, elapsed time: 76.971\n",
      "Imputing row 6801/18213 with 8 missing, elapsed time: 77.016\n",
      "Imputing row 6901/18213 with 0 missing, elapsed time: 77.065\n",
      "Imputing row 7001/18213 with 2 missing, elapsed time: 77.102\n",
      "Imputing row 7101/18213 with 7 missing, elapsed time: 77.147\n",
      "Imputing row 7201/18213 with 4 missing, elapsed time: 77.204\n",
      "Imputing row 7301/18213 with 16 missing, elapsed time: 77.247\n",
      "Imputing row 7401/18213 with 0 missing, elapsed time: 77.290\n",
      "Imputing row 7501/18213 with 0 missing, elapsed time: 77.333\n",
      "Imputing row 7601/18213 with 26 missing, elapsed time: 77.385\n",
      "Imputing row 7701/18213 with 3 missing, elapsed time: 77.425\n",
      "Imputing row 7801/18213 with 1 missing, elapsed time: 77.466\n",
      "Imputing row 7901/18213 with 12 missing, elapsed time: 77.506\n",
      "Imputing row 8001/18213 with 11 missing, elapsed time: 77.547\n",
      "Imputing row 8101/18213 with 7 missing, elapsed time: 77.631\n",
      "Imputing row 8201/18213 with 0 missing, elapsed time: 77.673\n",
      "Imputing row 8301/18213 with 3 missing, elapsed time: 77.714\n",
      "Imputing row 8401/18213 with 10 missing, elapsed time: 77.766\n",
      "Imputing row 8501/18213 with 4 missing, elapsed time: 77.820\n",
      "Imputing row 8601/18213 with 2 missing, elapsed time: 77.861\n",
      "Imputing row 8701/18213 with 0 missing, elapsed time: 77.900\n",
      "Imputing row 8801/18213 with 1 missing, elapsed time: 77.944\n",
      "Imputing row 8901/18213 with 8 missing, elapsed time: 77.991\n",
      "Imputing row 9001/18213 with 3 missing, elapsed time: 78.045\n",
      "Imputing row 9101/18213 with 5 missing, elapsed time: 78.088\n",
      "Imputing row 9201/18213 with 1 missing, elapsed time: 78.126\n",
      "Imputing row 9301/18213 with 5 missing, elapsed time: 78.157\n",
      "Imputing row 9401/18213 with 1 missing, elapsed time: 78.198\n",
      "Imputing row 9501/18213 with 0 missing, elapsed time: 78.241\n",
      "Imputing row 9601/18213 with 5 missing, elapsed time: 78.284\n",
      "Imputing row 9701/18213 with 2 missing, elapsed time: 78.325\n",
      "Imputing row 9801/18213 with 1 missing, elapsed time: 78.378\n",
      "Imputing row 9901/18213 with 14 missing, elapsed time: 78.426\n",
      "Imputing row 10001/18213 with 0 missing, elapsed time: 78.470\n",
      "Imputing row 10101/18213 with 9 missing, elapsed time: 78.510\n",
      "Imputing row 10201/18213 with 2 missing, elapsed time: 78.557\n",
      "Imputing row 10301/18213 with 6 missing, elapsed time: 78.608\n",
      "Imputing row 10401/18213 with 24 missing, elapsed time: 78.652\n",
      "Imputing row 10501/18213 with 12 missing, elapsed time: 78.689\n",
      "Imputing row 10601/18213 with 3 missing, elapsed time: 78.763\n",
      "Imputing row 10701/18213 with 2 missing, elapsed time: 78.808\n",
      "Imputing row 10801/18213 with 5 missing, elapsed time: 78.859\n",
      "Imputing row 10901/18213 with 19 missing, elapsed time: 78.915\n",
      "Imputing row 11001/18213 with 1 missing, elapsed time: 78.971\n",
      "Imputing row 11101/18213 with 4 missing, elapsed time: 79.022\n",
      "Imputing row 11201/18213 with 2 missing, elapsed time: 79.058\n",
      "Imputing row 11301/18213 with 8 missing, elapsed time: 79.093\n",
      "Imputing row 11401/18213 with 5 missing, elapsed time: 79.150\n",
      "Imputing row 11501/18213 with 0 missing, elapsed time: 79.201\n",
      "Imputing row 11601/18213 with 1 missing, elapsed time: 79.254\n",
      "Imputing row 11701/18213 with 2 missing, elapsed time: 79.297\n",
      "Imputing row 11801/18213 with 7 missing, elapsed time: 79.343\n",
      "Imputing row 11901/18213 with 11 missing, elapsed time: 79.376\n",
      "Imputing row 12001/18213 with 7 missing, elapsed time: 79.423\n",
      "Imputing row 12101/18213 with 1 missing, elapsed time: 79.475\n",
      "Imputing row 12201/18213 with 3 missing, elapsed time: 79.517\n",
      "Imputing row 12301/18213 with 16 missing, elapsed time: 79.561\n",
      "Imputing row 12401/18213 with 6 missing, elapsed time: 79.619\n",
      "Imputing row 12501/18213 with 4 missing, elapsed time: 79.678\n",
      "Imputing row 12601/18213 with 2 missing, elapsed time: 79.723\n",
      "Imputing row 12701/18213 with 1 missing, elapsed time: 79.766\n",
      "Imputing row 12801/18213 with 5 missing, elapsed time: 79.820\n",
      "Imputing row 12901/18213 with 12 missing, elapsed time: 79.859\n",
      "Imputing row 13001/18213 with 3 missing, elapsed time: 79.913\n",
      "Imputing row 13101/18213 with 0 missing, elapsed time: 79.955\n",
      "Imputing row 13201/18213 with 14 missing, elapsed time: 80.016\n",
      "Imputing row 13301/18213 with 0 missing, elapsed time: 80.058\n",
      "Imputing row 13401/18213 with 1 missing, elapsed time: 80.096\n",
      "Imputing row 13501/18213 with 0 missing, elapsed time: 80.132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 13601/18213 with 12 missing, elapsed time: 80.195\n",
      "Imputing row 13701/18213 with 12 missing, elapsed time: 80.237\n",
      "Imputing row 13801/18213 with 1 missing, elapsed time: 80.270\n",
      "Imputing row 13901/18213 with 2 missing, elapsed time: 80.315\n",
      "Imputing row 14001/18213 with 5 missing, elapsed time: 80.369\n",
      "Imputing row 14101/18213 with 5 missing, elapsed time: 80.420\n",
      "Imputing row 14201/18213 with 1 missing, elapsed time: 80.465\n",
      "Imputing row 14301/18213 with 1 missing, elapsed time: 80.514\n",
      "Imputing row 14401/18213 with 1 missing, elapsed time: 80.557\n",
      "Imputing row 14501/18213 with 7 missing, elapsed time: 80.589\n",
      "Imputing row 14601/18213 with 0 missing, elapsed time: 80.629\n",
      "Imputing row 14701/18213 with 11 missing, elapsed time: 80.662\n",
      "Imputing row 14801/18213 with 1 missing, elapsed time: 80.696\n",
      "Imputing row 14901/18213 with 9 missing, elapsed time: 80.754\n",
      "Imputing row 15001/18213 with 6 missing, elapsed time: 80.796\n",
      "Imputing row 15101/18213 with 9 missing, elapsed time: 80.840\n",
      "Imputing row 15201/18213 with 8 missing, elapsed time: 80.880\n",
      "Imputing row 15301/18213 with 2 missing, elapsed time: 80.936\n",
      "Imputing row 15401/18213 with 1 missing, elapsed time: 80.979\n",
      "Imputing row 15501/18213 with 9 missing, elapsed time: 81.021\n",
      "Imputing row 15601/18213 with 2 missing, elapsed time: 81.058\n",
      "Imputing row 15701/18213 with 6 missing, elapsed time: 81.095\n",
      "Imputing row 15801/18213 with 7 missing, elapsed time: 81.134\n",
      "Imputing row 15901/18213 with 8 missing, elapsed time: 81.172\n",
      "Imputing row 16001/18213 with 0 missing, elapsed time: 81.226\n",
      "Imputing row 16101/18213 with 1 missing, elapsed time: 81.271\n",
      "Imputing row 16201/18213 with 0 missing, elapsed time: 81.320\n",
      "Imputing row 16301/18213 with 0 missing, elapsed time: 81.358\n",
      "Imputing row 16401/18213 with 4 missing, elapsed time: 81.392\n",
      "Imputing row 16501/18213 with 1 missing, elapsed time: 81.428\n",
      "Imputing row 16601/18213 with 1 missing, elapsed time: 81.470\n",
      "Imputing row 16701/18213 with 2 missing, elapsed time: 81.518\n",
      "Imputing row 16801/18213 with 4 missing, elapsed time: 81.558\n",
      "Imputing row 16901/18213 with 5 missing, elapsed time: 81.607\n",
      "Imputing row 17001/18213 with 2 missing, elapsed time: 81.643\n",
      "Imputing row 17101/18213 with 1 missing, elapsed time: 81.678\n",
      "Imputing row 17201/18213 with 0 missing, elapsed time: 81.711\n",
      "Imputing row 17301/18213 with 1 missing, elapsed time: 81.747\n",
      "Imputing row 17401/18213 with 12 missing, elapsed time: 81.801\n",
      "Imputing row 17501/18213 with 14 missing, elapsed time: 81.844\n",
      "Imputing row 17601/18213 with 7 missing, elapsed time: 81.885\n",
      "Imputing row 17701/18213 with 0 missing, elapsed time: 81.922\n",
      "Imputing row 17801/18213 with 5 missing, elapsed time: 81.961\n",
      "Imputing row 17901/18213 with 7 missing, elapsed time: 81.998\n",
      "Imputing row 18001/18213 with 1 missing, elapsed time: 82.035\n",
      "Imputing row 18101/18213 with 1 missing, elapsed time: 82.070\n",
      "Imputing row 18201/18213 with 1 missing, elapsed time: 82.117\n"
     ]
    }
   ],
   "source": [
    "knn_imputer = KNN(k=5)\n",
    "df_imputed = pd.DataFrame(knn_imputer.fit_transform(df[factors]), columns=factors, index=df[factors].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27368450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_imputed, df[['year', 'targeted',\"market_cap\",\"bic_level_2\",\"bic_level_3\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7db64b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 0s 313us/step\n",
      "82/82 [==============================] - 0s 321us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.484504  0.478142\n",
      "1        RF   0.609897  0.624728\n",
      "2       XGB   0.638907  0.609764\n",
      "3      LGBM   0.651705  0.619696\n",
      "4  CatBoost   0.647292  0.627056\n",
      "5        NN   0.924070  0.591044\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result1 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfc07a",
   "metadata": {},
   "source": [
    "### 1.2. oversampling with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352f181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Apply ADASYN oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9241c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797/797 [==============================] - 0s 305us/step\n",
      "82/82 [==============================] - 0s 311us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.575810  0.513087\n",
      "1        RF   0.999408  0.603214\n",
      "2       XGB   0.997633  0.606136\n",
      "3      LGBM   0.995711  0.639186\n",
      "4  CatBoost   0.995480  0.602602\n",
      "5        NN   0.998786  0.578502\n"
     ]
    }
   ],
   "source": [
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result2 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18d764",
   "metadata": {},
   "source": [
    "## 2.1 with column creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506b9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../database/2016-2022.csv')\n",
    "df['ev_ebitda'] = np.where((df['ev'] != 0) & (df['ebitda'] != 0), df['ev'] / df['ebitda'], np.nan)\n",
    "df['ev_ebit'] = np.where((df['ev'] != 0) & (df['ebit'] != 0), df['ev'] / df['ebit'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6775b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in non_ratio_variables:\n",
    "    \n",
    "    # 1. _percentile\n",
    "    percentile_col = col + '_percentile'\n",
    "    df[percentile_col] = df.groupby('year')[col].transform(lambda x: x.rank(pct=True) * 100)\n",
    "    df[percentile_col].fillna(50, inplace=True)\n",
    "    \n",
    "    # 2. _10bins_percentile\n",
    "    df['market_cap_bins'] = df.groupby('year')['market_cap'].transform(lambda x: pd.cut(x, bins=10))\n",
    "    percentile_10bins_col = col + '_10bins_percentile'\n",
    "    df[percentile_10bins_col] = df.groupby(['year', 'market_cap_bins'])[col].transform(lambda x: x.rank(pct=True) * 100)\n",
    "    df[percentile_10bins_col].fillna(50, inplace=True)\n",
    "    df.drop('market_cap_bins', axis=1, inplace=True)\n",
    "\n",
    "    # 3. _10bins_normalized\n",
    "    df['market_cap_bins'] = df.groupby('year')['market_cap'].transform(lambda x: pd.qcut(x, 10, labels=False, duplicates='drop'))\n",
    "    normalized_col = col + '_10bins_normalized'\n",
    "    df[normalized_col] = df.groupby(['year', 'market_cap_bins'])[col].transform(lambda x: (x - x.mean()) / x.std())\n",
    "    df[normalized_col].fillna(0, inplace=True)\n",
    "    df.drop('market_cap_bins', axis=1, inplace=True)\n",
    "    \n",
    "    # 4. _div_market_cap\n",
    "    div_market_cap_col = col + '_div_market_cap'\n",
    "    df[div_market_cap_col] = df[col] / df['market_cap']\n",
    "    \n",
    "    # 5. _div_log_market_cap\n",
    "    df['log_market_cap'] = np.log(df['market_cap'])\n",
    "    div_log_market_cap_col = col + '_div_log_market_cap'\n",
    "    df[div_log_market_cap_col] = df[col] / df['log_market_cap']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1626788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentile(group):\n",
    "    if len(group) < 10:\n",
    "        return pd.Series([None] * len(group), index=group.index, dtype=float)\n",
    "    return group.rank(pct=True) * 100\n",
    "\n",
    "def normalize(group):\n",
    "    if len(group) < 10:\n",
    "        return pd.Series([None] * len(group), index=group.index, dtype=float)\n",
    "    return (group - group.mean()) / group.std()\n",
    "\n",
    "for col in ratio_variables:\n",
    "    percentile_col = col + '_industry_peers_percentile'\n",
    "    df[percentile_col] = df.groupby(['year', 'bic_level_3'])[col].transform(compute_percentile)\n",
    "    mask = df[percentile_col].isna()\n",
    "    df.loc[mask, percentile_col] = df[mask].groupby(['year', 'bic_level_2'])[col].transform(compute_percentile)\n",
    "    df[percentile_col].fillna(50, inplace=True)\n",
    "    df[percentile_col] = df[percentile_col].astype(float)\n",
    "    normalized_col = col + '_industry_peers_normalized'\n",
    "    df[normalized_col] = df.groupby(['year', 'bic_level_3'])[col].transform(normalize)\n",
    "    mask = df[normalized_col].isna()\n",
    "    df.loc[mask, normalized_col] = df[mask].groupby(['year', 'bic_level_2'])[col].transform(normalize)\n",
    "    df[normalized_col].fillna(0, inplace=True)\n",
    "    df[normalized_col] = df[normalized_col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "900daeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = []\n",
    "for col in non_ratio_variables:\n",
    "    factors.extend([\n",
    "        col,\n",
    "        f'{col}_percentile',\n",
    "        f'{col}_10bins_percentile',\n",
    "        f'{col}_10bins_normalized',\n",
    "        f'{col}_div_market_cap',\n",
    "        f'{col}_div_log_market_cap'\n",
    "    ])\n",
    "\n",
    "for col in ratio_variables:\n",
    "    factors.extend([\n",
    "        col,\n",
    "        f'{col}_industry_peers_percentile',\n",
    "        f'{col}_industry_peers_normalized'\n",
    "    ])\n",
    "\n",
    "factors = factors + binary + technical_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b385d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/18213 with 7 missing, elapsed time: 164.340\n",
      "Imputing row 101/18213 with 6 missing, elapsed time: 164.470\n",
      "Imputing row 201/18213 with 7 missing, elapsed time: 164.555\n",
      "Imputing row 301/18213 with 8 missing, elapsed time: 164.632\n",
      "Imputing row 401/18213 with 1 missing, elapsed time: 164.705\n",
      "Imputing row 501/18213 with 0 missing, elapsed time: 164.802\n",
      "Imputing row 601/18213 with 1 missing, elapsed time: 164.893\n",
      "Imputing row 701/18213 with 14 missing, elapsed time: 164.967\n",
      "Imputing row 801/18213 with 8 missing, elapsed time: 165.039\n",
      "Imputing row 901/18213 with 1 missing, elapsed time: 165.112\n",
      "Imputing row 1001/18213 with 7 missing, elapsed time: 165.205\n",
      "Imputing row 1101/18213 with 6 missing, elapsed time: 165.297\n",
      "Imputing row 1201/18213 with 7 missing, elapsed time: 165.384\n",
      "Imputing row 1301/18213 with 16 missing, elapsed time: 165.470\n",
      "Imputing row 1401/18213 with 0 missing, elapsed time: 165.538\n",
      "Imputing row 1501/18213 with 31 missing, elapsed time: 165.608\n",
      "Imputing row 1601/18213 with 14 missing, elapsed time: 165.687\n",
      "Imputing row 1701/18213 with 1 missing, elapsed time: 165.753\n",
      "Imputing row 1801/18213 with 29 missing, elapsed time: 165.838\n",
      "Imputing row 1901/18213 with 1 missing, elapsed time: 165.931\n",
      "Imputing row 2001/18213 with 25 missing, elapsed time: 166.015\n",
      "Imputing row 2101/18213 with 16 missing, elapsed time: 166.097\n",
      "Imputing row 2201/18213 with 22 missing, elapsed time: 166.173\n",
      "Imputing row 2301/18213 with 4 missing, elapsed time: 166.261\n",
      "Imputing row 2401/18213 with 10 missing, elapsed time: 166.349\n",
      "Imputing row 2501/18213 with 6 missing, elapsed time: 166.437\n",
      "Imputing row 2601/18213 with 11 missing, elapsed time: 166.510\n",
      "Imputing row 2701/18213 with 13 missing, elapsed time: 166.583\n",
      "Imputing row 2801/18213 with 6 missing, elapsed time: 166.705\n",
      "Imputing row 2901/18213 with 1 missing, elapsed time: 166.780\n",
      "Imputing row 3001/18213 with 24 missing, elapsed time: 166.850\n",
      "Imputing row 3101/18213 with 12 missing, elapsed time: 166.921\n",
      "Imputing row 3201/18213 with 6 missing, elapsed time: 167.010\n",
      "Imputing row 3301/18213 with 8 missing, elapsed time: 167.085\n",
      "Imputing row 3401/18213 with 14 missing, elapsed time: 167.168\n",
      "Imputing row 3501/18213 with 26 missing, elapsed time: 167.242\n",
      "Imputing row 3601/18213 with 6 missing, elapsed time: 167.317\n",
      "Imputing row 3701/18213 with 9 missing, elapsed time: 167.422\n",
      "Imputing row 3801/18213 with 11 missing, elapsed time: 167.497\n",
      "Imputing row 3901/18213 with 10 missing, elapsed time: 167.583\n",
      "Imputing row 4001/18213 with 14 missing, elapsed time: 167.656\n",
      "Imputing row 4101/18213 with 19 missing, elapsed time: 167.719\n",
      "Imputing row 4201/18213 with 24 missing, elapsed time: 167.778\n",
      "Imputing row 4301/18213 with 0 missing, elapsed time: 167.843\n",
      "Imputing row 4401/18213 with 8 missing, elapsed time: 167.922\n",
      "Imputing row 4501/18213 with 23 missing, elapsed time: 168.001\n",
      "Imputing row 4601/18213 with 3 missing, elapsed time: 168.088\n",
      "Imputing row 4701/18213 with 7 missing, elapsed time: 168.173\n",
      "Imputing row 4801/18213 with 24 missing, elapsed time: 168.236\n",
      "Imputing row 4901/18213 with 30 missing, elapsed time: 168.311\n",
      "Imputing row 5001/18213 with 6 missing, elapsed time: 168.392\n",
      "Imputing row 5101/18213 with 3 missing, elapsed time: 168.461\n",
      "Imputing row 5201/18213 with 10 missing, elapsed time: 168.537\n",
      "Imputing row 5301/18213 with 24 missing, elapsed time: 168.612\n",
      "Imputing row 5401/18213 with 70 missing, elapsed time: 168.710\n",
      "Imputing row 5501/18213 with 1 missing, elapsed time: 168.827\n",
      "Imputing row 5601/18213 with 33 missing, elapsed time: 168.907\n",
      "Imputing row 5701/18213 with 1 missing, elapsed time: 168.972\n",
      "Imputing row 5801/18213 with 9 missing, elapsed time: 169.081\n",
      "Imputing row 5901/18213 with 1 missing, elapsed time: 169.176\n",
      "Imputing row 6001/18213 with 14 missing, elapsed time: 169.248\n",
      "Imputing row 6101/18213 with 9 missing, elapsed time: 169.324\n",
      "Imputing row 6201/18213 with 4 missing, elapsed time: 169.406\n",
      "Imputing row 6301/18213 with 15 missing, elapsed time: 169.504\n",
      "Imputing row 6401/18213 with 20 missing, elapsed time: 169.599\n",
      "Imputing row 6501/18213 with 19 missing, elapsed time: 169.673\n",
      "Imputing row 6601/18213 with 16 missing, elapsed time: 169.747\n",
      "Imputing row 6701/18213 with 0 missing, elapsed time: 169.813\n",
      "Imputing row 6801/18213 with 18 missing, elapsed time: 169.891\n",
      "Imputing row 6901/18213 with 0 missing, elapsed time: 169.964\n",
      "Imputing row 7001/18213 with 6 missing, elapsed time: 170.022\n",
      "Imputing row 7101/18213 with 17 missing, elapsed time: 170.104\n",
      "Imputing row 7201/18213 with 8 missing, elapsed time: 170.190\n",
      "Imputing row 7301/18213 with 32 missing, elapsed time: 170.273\n",
      "Imputing row 7401/18213 with 0 missing, elapsed time: 170.356\n",
      "Imputing row 7501/18213 with 0 missing, elapsed time: 170.437\n",
      "Imputing row 7601/18213 with 40 missing, elapsed time: 170.529\n",
      "Imputing row 7701/18213 with 3 missing, elapsed time: 170.602\n",
      "Imputing row 7801/18213 with 1 missing, elapsed time: 170.681\n",
      "Imputing row 7901/18213 with 24 missing, elapsed time: 170.756\n",
      "Imputing row 8001/18213 with 13 missing, elapsed time: 170.831\n",
      "Imputing row 8101/18213 with 15 missing, elapsed time: 170.996\n",
      "Imputing row 8201/18213 with 0 missing, elapsed time: 171.076\n",
      "Imputing row 8301/18213 with 5 missing, elapsed time: 171.148\n",
      "Imputing row 8401/18213 with 10 missing, elapsed time: 171.243\n",
      "Imputing row 8501/18213 with 4 missing, elapsed time: 171.344\n",
      "Imputing row 8601/18213 with 2 missing, elapsed time: 171.422\n",
      "Imputing row 8701/18213 with 0 missing, elapsed time: 171.503\n",
      "Imputing row 8801/18213 with 1 missing, elapsed time: 171.570\n",
      "Imputing row 8901/18213 with 18 missing, elapsed time: 171.695\n",
      "Imputing row 9001/18213 with 7 missing, elapsed time: 171.812\n",
      "Imputing row 9101/18213 with 13 missing, elapsed time: 171.890\n",
      "Imputing row 9201/18213 with 1 missing, elapsed time: 171.963\n",
      "Imputing row 9301/18213 with 11 missing, elapsed time: 172.021\n",
      "Imputing row 9401/18213 with 1 missing, elapsed time: 172.095\n",
      "Imputing row 9501/18213 with 0 missing, elapsed time: 172.161\n",
      "Imputing row 9601/18213 with 9 missing, elapsed time: 172.228\n",
      "Imputing row 9701/18213 with 6 missing, elapsed time: 172.311\n",
      "Imputing row 9801/18213 with 1 missing, elapsed time: 172.413\n",
      "Imputing row 9901/18213 with 30 missing, elapsed time: 172.500\n",
      "Imputing row 10001/18213 with 0 missing, elapsed time: 172.582\n",
      "Imputing row 10101/18213 with 13 missing, elapsed time: 172.657\n",
      "Imputing row 10201/18213 with 2 missing, elapsed time: 172.747\n",
      "Imputing row 10301/18213 with 10 missing, elapsed time: 172.821\n",
      "Imputing row 10401/18213 with 36 missing, elapsed time: 172.900\n",
      "Imputing row 10501/18213 with 24 missing, elapsed time: 172.968\n",
      "Imputing row 10601/18213 with 7 missing, elapsed time: 173.095\n",
      "Imputing row 10701/18213 with 6 missing, elapsed time: 173.181\n",
      "Imputing row 10801/18213 with 9 missing, elapsed time: 173.278\n",
      "Imputing row 10901/18213 with 23 missing, elapsed time: 173.357\n",
      "Imputing row 11001/18213 with 1 missing, elapsed time: 173.458\n",
      "Imputing row 11101/18213 with 8 missing, elapsed time: 173.549\n",
      "Imputing row 11201/18213 with 6 missing, elapsed time: 173.614\n",
      "Imputing row 11301/18213 with 18 missing, elapsed time: 173.678\n",
      "Imputing row 11401/18213 with 9 missing, elapsed time: 173.753\n",
      "Imputing row 11501/18213 with 0 missing, elapsed time: 173.855\n",
      "Imputing row 11601/18213 with 1 missing, elapsed time: 173.955\n",
      "Imputing row 11701/18213 with 2 missing, elapsed time: 174.040\n",
      "Imputing row 11801/18213 with 11 missing, elapsed time: 174.131\n",
      "Imputing row 11901/18213 with 21 missing, elapsed time: 174.194\n",
      "Imputing row 12001/18213 with 11 missing, elapsed time: 174.279\n",
      "Imputing row 12101/18213 with 1 missing, elapsed time: 174.363\n",
      "Imputing row 12201/18213 with 7 missing, elapsed time: 174.444\n",
      "Imputing row 12301/18213 with 32 missing, elapsed time: 174.534\n",
      "Imputing row 12401/18213 with 6 missing, elapsed time: 174.655\n",
      "Imputing row 12501/18213 with 8 missing, elapsed time: 174.755\n",
      "Imputing row 12601/18213 with 6 missing, elapsed time: 174.841\n",
      "Imputing row 12701/18213 with 1 missing, elapsed time: 174.923\n",
      "Imputing row 12801/18213 with 9 missing, elapsed time: 175.022\n",
      "Imputing row 12901/18213 with 24 missing, elapsed time: 175.096\n",
      "Imputing row 13001/18213 with 7 missing, elapsed time: 175.170\n",
      "Imputing row 13101/18213 with 0 missing, elapsed time: 175.241\n",
      "Imputing row 13201/18213 with 30 missing, elapsed time: 175.348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 13301/18213 with 0 missing, elapsed time: 175.414\n",
      "Imputing row 13401/18213 with 1 missing, elapsed time: 175.474\n",
      "Imputing row 13501/18213 with 0 missing, elapsed time: 175.527\n",
      "Imputing row 13601/18213 with 24 missing, elapsed time: 175.612\n",
      "Imputing row 13701/18213 with 24 missing, elapsed time: 175.680\n",
      "Imputing row 13801/18213 with 1 missing, elapsed time: 175.734\n",
      "Imputing row 13901/18213 with 2 missing, elapsed time: 175.815\n",
      "Imputing row 14001/18213 with 13 missing, elapsed time: 175.878\n",
      "Imputing row 14101/18213 with 11 missing, elapsed time: 175.961\n",
      "Imputing row 14201/18213 with 1 missing, elapsed time: 176.034\n",
      "Imputing row 14301/18213 with 1 missing, elapsed time: 176.101\n",
      "Imputing row 14401/18213 with 3 missing, elapsed time: 176.175\n",
      "Imputing row 14501/18213 with 17 missing, elapsed time: 176.225\n",
      "Imputing row 14601/18213 with 0 missing, elapsed time: 176.286\n",
      "Imputing row 14701/18213 with 19 missing, elapsed time: 176.336\n",
      "Imputing row 14801/18213 with 1 missing, elapsed time: 176.394\n",
      "Imputing row 14901/18213 with 21 missing, elapsed time: 176.474\n",
      "Imputing row 15001/18213 with 6 missing, elapsed time: 176.544\n",
      "Imputing row 15101/18213 with 13 missing, elapsed time: 176.613\n",
      "Imputing row 15201/18213 with 16 missing, elapsed time: 176.673\n",
      "Imputing row 15301/18213 with 6 missing, elapsed time: 176.740\n",
      "Imputing row 15401/18213 with 1 missing, elapsed time: 176.810\n",
      "Imputing row 15501/18213 with 17 missing, elapsed time: 176.876\n",
      "Imputing row 15601/18213 with 2 missing, elapsed time: 176.934\n",
      "Imputing row 15701/18213 with 8 missing, elapsed time: 176.988\n",
      "Imputing row 15801/18213 with 13 missing, elapsed time: 177.048\n",
      "Imputing row 15901/18213 with 12 missing, elapsed time: 177.107\n",
      "Imputing row 16001/18213 with 0 missing, elapsed time: 177.170\n",
      "Imputing row 16101/18213 with 1 missing, elapsed time: 177.239\n",
      "Imputing row 16201/18213 with 0 missing, elapsed time: 177.324\n",
      "Imputing row 16301/18213 with 0 missing, elapsed time: 177.385\n",
      "Imputing row 16401/18213 with 4 missing, elapsed time: 177.439\n",
      "Imputing row 16501/18213 with 1 missing, elapsed time: 177.494\n",
      "Imputing row 16601/18213 with 1 missing, elapsed time: 177.573\n",
      "Imputing row 16701/18213 with 2 missing, elapsed time: 177.652\n",
      "Imputing row 16801/18213 with 4 missing, elapsed time: 177.709\n",
      "Imputing row 16901/18213 with 5 missing, elapsed time: 177.775\n",
      "Imputing row 17001/18213 with 2 missing, elapsed time: 177.829\n",
      "Imputing row 17101/18213 with 1 missing, elapsed time: 177.883\n",
      "Imputing row 17201/18213 with 0 missing, elapsed time: 177.945\n",
      "Imputing row 17301/18213 with 1 missing, elapsed time: 178.004\n",
      "Imputing row 17401/18213 with 24 missing, elapsed time: 178.078\n",
      "Imputing row 17501/18213 with 30 missing, elapsed time: 178.147\n",
      "Imputing row 17601/18213 with 15 missing, elapsed time: 178.209\n",
      "Imputing row 17701/18213 with 0 missing, elapsed time: 178.269\n",
      "Imputing row 17801/18213 with 13 missing, elapsed time: 178.349\n",
      "Imputing row 17901/18213 with 15 missing, elapsed time: 178.408\n",
      "Imputing row 18001/18213 with 1 missing, elapsed time: 178.466\n",
      "Imputing row 18101/18213 with 1 missing, elapsed time: 178.525\n",
      "Imputing row 18201/18213 with 1 missing, elapsed time: 178.582\n"
     ]
    }
   ],
   "source": [
    "knn_imputer = KNN(k=5)\n",
    "df_imputed = pd.DataFrame(knn_imputer.fit_transform(df[factors]), columns=factors, index=df[factors].index)\n",
    "df = pd.concat([df_imputed, df[['year', 'targeted',\"market_cap\",\"bic_level_2\",\"bic_level_3\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63879812",
   "metadata": {},
   "source": [
    "### 2.1. raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8466b0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 0s 347us/step\n",
      "82/82 [==============================] - 0s 349us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.524444  0.505737\n",
      "1        RF   0.625340  0.558816\n",
      "2       XGB   0.655281  0.657532\n",
      "3      LGBM   0.650973  0.670624\n",
      "4  CatBoost   0.643042  0.674861\n",
      "5        NN   0.942524  0.619263\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result3 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c2684",
   "metadata": {},
   "source": [
    "### 2.2. oversampling the ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a233d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798/798 [==============================] - 0s 320us/step\n",
      "82/82 [==============================] - 0s 428us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.595549  0.562235\n",
      "1        RF   0.999778  0.613273\n",
      "2       XGB   0.998425  0.641217\n",
      "3      LGBM   0.996706  0.619914\n",
      "4  CatBoost   0.994914  0.601928\n",
      "5        NN   0.999998  0.643285\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Apply ADASYN oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result4 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
