{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2acb5f",
   "metadata": {},
   "source": [
    "# Structure\n",
    "\n",
    "Imputation with kNN\n",
    "- 1. no column creation\n",
    "    - 1.1. raw data\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "    - 1.2. ADASYN imputation\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "- 2. yes column creation\n",
    "    - 2.1. raw data\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "    - 2.2. ADASYN imputation\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a391119",
   "metadata": {},
   "source": [
    "## Load the data and the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46fdda19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Oct 25 04:08:48 PM: Encountered unexpected exception importing solver CVXOPT:\n",
      "ImportError(\"dlopen(/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/liblapack.3.dylib\\n  Referenced from: <E25E40AB-7857-39B9-8DE7-28B7B0E4806B> /Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so\\n  Reason: tried: '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/usr/local/lib/liblapack.3.dylib' (no such file), '/usr/lib/liblapack.3.dylib' (no such file, not in dyld cache)\")\n",
      "(CVXPY) Oct 25 04:08:48 PM: Encountered unexpected exception importing solver GLPK:\n",
      "ImportError(\"dlopen(/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/liblapack.3.dylib\\n  Referenced from: <E25E40AB-7857-39B9-8DE7-28B7B0E4806B> /Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so\\n  Reason: tried: '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/usr/local/lib/liblapack.3.dylib' (no such file), '/usr/lib/liblapack.3.dylib' (no such file, not in dyld cache)\")\n",
      "(CVXPY) Oct 25 04:08:48 PM: Encountered unexpected exception importing solver GLPK_MI:\n",
      "ImportError(\"dlopen(/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/liblapack.3.dylib\\n  Referenced from: <E25E40AB-7857-39B9-8DE7-28B7B0E4806B> /Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/base.cpython-311-darwin.so\\n  Reason: tried: '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/lib/python3.11/site-packages/cvxopt/../../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/usr/local/lib/liblapack.3.dylib' (no such file), '/usr/lib/liblapack.3.dylib' (no such file, not in dyld cache)\")\n",
      "(CVXPY) Oct 25 04:08:48 PM: Encountered unexpected exception importing solver SCS:\n",
      "ImportError(\"dlopen(/Users/minwukim/anaconda3/lib/python3.11/site-packages/_scs_direct.cpython-311-darwin.so, 0x0002): Library not loaded: @rpath/liblapack.3.dylib\\n  Referenced from: <A08A2CF9-B9A1-393C-A32E-68987F02C61E> /Users/minwukim/anaconda3/lib/python3.11/site-packages/_scs_direct.cpython-311-darwin.so\\n  Reason: tried: '/Users/minwukim/anaconda3/lib/python3.11/site-packages/../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/lib/python3.11/site-packages/../../liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/Users/minwukim/anaconda3/bin/../lib/liblapack.3.dylib' (no such file), '/usr/local/lib/liblapack.3.dylib' (no such file), '/usr/lib/liblapack.3.dylib' (no such file, not in dyld cache)\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from fancyimpute import KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce798c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../database/2016-2022_semantic_imputation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57082e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = ['unequal_voting','classified_board_system','poison_pill','operating_margin_below_3y_average']\n",
    "non_ratio_variables = [\n",
    "    \"capex\",\n",
    "    \"net_capex\",\n",
    "    \"short_term_wc\",\n",
    "    \"long_term_wc\",\n",
    "    \"modified_wc\",\n",
    "    \"ebitda\",\n",
    "    \"ebit\",\n",
    "    \"net_income\",\n",
    "    \"net_debt\",\n",
    "    \"ev\",\n",
    "    \"repurchase\",\n",
    "    \"board_size\",\n",
    "    \"net_repurchase\",\n",
    "    \"total_compensation_to_executives\",\n",
    "    \"total_compensation_to_board_members\",\n",
    "    \"dividend_to_common\",\n",
    "    \"dividend_to_preferred\"\n",
    "]\n",
    "\n",
    "df['ev_ebitda'] = np.where((df['ev'] != 0) & (df['ebitda'] != 0), df['ev'] / df['ebitda'], np.nan)\n",
    "df['ev_ebit'] = np.where((df['ev'] != 0) & (df['ebit'] != 0), df['ev'] / df['ebit'], np.nan)\n",
    "\n",
    "ratio_variables = [\n",
    "    \"ebitda_margin\",\n",
    "    \"operating_margin\",\n",
    "    \"sales_to_total_assets\",\n",
    "    \"roe\",\n",
    "    \"normalized_roe\",\n",
    "    \"operating_roe\",\n",
    "    \"operating_roic\",\n",
    "    \"eps_adjusted_diluted\",\n",
    "    \"ev_to_sales\",\n",
    "    \"tobin_q_ratio\",\n",
    "    \"pb_ratio\",\n",
    "    \"pe_ratio\",\n",
    "    \"fcf_to_equity\",\n",
    "    \"sales_growth_rate\",\n",
    "    \"dividend_per_share\",\n",
    "    \"dividend_payout_ratio\",\n",
    "    \"asset_to_equity\",\n",
    "    \"cash_conversion_cycle\",\n",
    "    \"ev_ebitda\",\n",
    "    \"ev_ebit\",\n",
    "]\n",
    "\n",
    "technical_variables = [\n",
    "    \"free_float_percentage\",\n",
    "    \"rsi_14d\",\n",
    "    \"rsi_30d\",\n",
    "    \"volatility_30d\",\n",
    "    \"volatility_90d\",\n",
    "    \"volatility_180d\",\n",
    "    \"volume_30d_average_to_outstanding\",\n",
    "    \"insider_shares_percentage\",\n",
    "    \"institution_ownership_percentage\",\n",
    "    \"ceo_tenure\",\n",
    "    \"total_return_5y\",\n",
    "    \"total_return_4y\",\n",
    "    \"total_return_3y\",\n",
    "    \"total_return_2y\",\n",
    "    \"total_return_1y\",\n",
    "    \"total_return_6m\",\n",
    "    \"total_return_3m\",\n",
    "    \"employee_growth_rate\",\n",
    "    \"fcf_yield\"\n",
    "]\n",
    "\n",
    "supportive = [\"bic_level_2\",\"bic_level_3\",\"market_cap\"]\n",
    "factors = binary + non_ratio_variables + ratio_variables + technical_variables\n",
    "\n",
    "df[\"bic_level_2\"] = df[\"bic_level_2\"].astype('category')\n",
    "df[\"bic_level_3\"] = df[\"bic_level_3\"].astype('category')\n",
    "\n",
    "# factors.append(\"targeted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897f954",
   "metadata": {},
   "source": [
    "## 1. No column creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1007d1",
   "metadata": {},
   "source": [
    "### 1.1. raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66a501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/18213 with 5 missing, elapsed time: 75.085\n",
      "Imputing row 101/18213 with 2 missing, elapsed time: 75.157\n",
      "Imputing row 201/18213 with 3 missing, elapsed time: 75.196\n",
      "Imputing row 301/18213 with 4 missing, elapsed time: 75.228\n",
      "Imputing row 401/18213 with 1 missing, elapsed time: 75.263\n",
      "Imputing row 501/18213 with 0 missing, elapsed time: 75.309\n",
      "Imputing row 601/18213 with 1 missing, elapsed time: 75.364\n",
      "Imputing row 701/18213 with 5 missing, elapsed time: 75.398\n",
      "Imputing row 801/18213 with 3 missing, elapsed time: 75.432\n",
      "Imputing row 901/18213 with 1 missing, elapsed time: 75.467\n",
      "Imputing row 1001/18213 with 3 missing, elapsed time: 75.510\n",
      "Imputing row 1101/18213 with 2 missing, elapsed time: 75.553\n",
      "Imputing row 1201/18213 with 3 missing, elapsed time: 75.593\n",
      "Imputing row 1301/18213 with 6 missing, elapsed time: 75.648\n",
      "Imputing row 1401/18213 with 0 missing, elapsed time: 75.680\n",
      "Imputing row 1501/18213 with 14 missing, elapsed time: 75.711\n",
      "Imputing row 1601/18213 with 5 missing, elapsed time: 75.748\n",
      "Imputing row 1701/18213 with 1 missing, elapsed time: 75.779\n",
      "Imputing row 1801/18213 with 14 missing, elapsed time: 75.819\n",
      "Imputing row 1901/18213 with 1 missing, elapsed time: 75.857\n",
      "Imputing row 2001/18213 with 18 missing, elapsed time: 75.895\n",
      "Imputing row 2101/18213 with 6 missing, elapsed time: 75.934\n",
      "Imputing row 2201/18213 with 11 missing, elapsed time: 75.970\n",
      "Imputing row 2301/18213 with 1 missing, elapsed time: 76.028\n",
      "Imputing row 2401/18213 with 5 missing, elapsed time: 76.064\n",
      "Imputing row 2501/18213 with 2 missing, elapsed time: 76.103\n",
      "Imputing row 2601/18213 with 6 missing, elapsed time: 76.139\n",
      "Imputing row 2701/18213 with 8 missing, elapsed time: 76.174\n",
      "Imputing row 2801/18213 with 2 missing, elapsed time: 76.239\n",
      "Imputing row 2901/18213 with 1 missing, elapsed time: 76.275\n",
      "Imputing row 3001/18213 with 8 missing, elapsed time: 76.310\n",
      "Imputing row 3101/18213 with 7 missing, elapsed time: 76.352\n",
      "Imputing row 3201/18213 with 1 missing, elapsed time: 76.394\n",
      "Imputing row 3301/18213 with 4 missing, elapsed time: 76.432\n",
      "Imputing row 3401/18213 with 9 missing, elapsed time: 76.468\n",
      "Imputing row 3501/18213 with 13 missing, elapsed time: 76.504\n",
      "Imputing row 3601/18213 with 2 missing, elapsed time: 76.541\n",
      "Imputing row 3701/18213 with 4 missing, elapsed time: 76.591\n",
      "Imputing row 3801/18213 with 5 missing, elapsed time: 76.626\n",
      "Imputing row 3901/18213 with 6 missing, elapsed time: 76.667\n",
      "Imputing row 4001/18213 with 6 missing, elapsed time: 76.704\n",
      "Imputing row 4101/18213 with 9 missing, elapsed time: 76.735\n",
      "Imputing row 4201/18213 with 11 missing, elapsed time: 76.784\n",
      "Imputing row 4301/18213 with 0 missing, elapsed time: 76.822\n",
      "Imputing row 4401/18213 with 4 missing, elapsed time: 76.860\n",
      "Imputing row 4501/18213 with 8 missing, elapsed time: 76.897\n",
      "Imputing row 4601/18213 with 0 missing, elapsed time: 76.937\n",
      "Imputing row 4701/18213 with 3 missing, elapsed time: 76.977\n",
      "Imputing row 4801/18213 with 11 missing, elapsed time: 77.008\n",
      "Imputing row 4901/18213 with 15 missing, elapsed time: 77.046\n",
      "Imputing row 5001/18213 with 2 missing, elapsed time: 77.093\n",
      "Imputing row 5101/18213 with 2 missing, elapsed time: 77.139\n",
      "Imputing row 5201/18213 with 6 missing, elapsed time: 77.176\n",
      "Imputing row 5301/18213 with 11 missing, elapsed time: 77.213\n",
      "Imputing row 5401/18213 with 42 missing, elapsed time: 77.264\n",
      "Imputing row 5501/18213 with 1 missing, elapsed time: 77.323\n",
      "Imputing row 5601/18213 with 18 missing, elapsed time: 77.378\n",
      "Imputing row 5701/18213 with 1 missing, elapsed time: 77.411\n",
      "Imputing row 5801/18213 with 5 missing, elapsed time: 77.462\n",
      "Imputing row 5901/18213 with 1 missing, elapsed time: 77.504\n",
      "Imputing row 6001/18213 with 5 missing, elapsed time: 77.538\n",
      "Imputing row 6101/18213 with 4 missing, elapsed time: 77.573\n",
      "Imputing row 6201/18213 with 1 missing, elapsed time: 77.613\n",
      "Imputing row 6301/18213 with 8 missing, elapsed time: 77.658\n",
      "Imputing row 6401/18213 with 13 missing, elapsed time: 77.704\n",
      "Imputing row 6501/18213 with 8 missing, elapsed time: 77.760\n",
      "Imputing row 6601/18213 with 6 missing, elapsed time: 77.795\n",
      "Imputing row 6701/18213 with 0 missing, elapsed time: 77.825\n",
      "Imputing row 6801/18213 with 8 missing, elapsed time: 77.862\n",
      "Imputing row 6901/18213 with 0 missing, elapsed time: 77.898\n",
      "Imputing row 7001/18213 with 2 missing, elapsed time: 77.926\n",
      "Imputing row 7101/18213 with 7 missing, elapsed time: 77.962\n",
      "Imputing row 7201/18213 with 4 missing, elapsed time: 78.018\n",
      "Imputing row 7301/18213 with 14 missing, elapsed time: 78.055\n",
      "Imputing row 7401/18213 with 0 missing, elapsed time: 78.093\n",
      "Imputing row 7501/18213 with 0 missing, elapsed time: 78.129\n",
      "Imputing row 7601/18213 with 25 missing, elapsed time: 78.172\n",
      "Imputing row 7701/18213 with 2 missing, elapsed time: 78.219\n",
      "Imputing row 7801/18213 with 1 missing, elapsed time: 78.256\n",
      "Imputing row 7901/18213 with 11 missing, elapsed time: 78.292\n",
      "Imputing row 8001/18213 with 11 missing, elapsed time: 78.328\n",
      "Imputing row 8101/18213 with 4 missing, elapsed time: 78.402\n",
      "Imputing row 8201/18213 with 0 missing, elapsed time: 78.440\n",
      "Imputing row 8301/18213 with 3 missing, elapsed time: 78.476\n",
      "Imputing row 8401/18213 with 9 missing, elapsed time: 78.542\n",
      "Imputing row 8501/18213 with 4 missing, elapsed time: 78.591\n",
      "Imputing row 8601/18213 with 2 missing, elapsed time: 78.627\n",
      "Imputing row 8701/18213 with 0 missing, elapsed time: 78.662\n",
      "Imputing row 8801/18213 with 1 missing, elapsed time: 78.694\n",
      "Imputing row 8901/18213 with 4 missing, elapsed time: 78.735\n",
      "Imputing row 9001/18213 with 3 missing, elapsed time: 78.901\n",
      "Imputing row 9101/18213 with 5 missing, elapsed time: 78.944\n",
      "Imputing row 9201/18213 with 1 missing, elapsed time: 78.978\n",
      "Imputing row 9301/18213 with 5 missing, elapsed time: 79.005\n",
      "Imputing row 9401/18213 with 1 missing, elapsed time: 79.041\n",
      "Imputing row 9501/18213 with 0 missing, elapsed time: 79.074\n",
      "Imputing row 9601/18213 with 5 missing, elapsed time: 79.107\n",
      "Imputing row 9701/18213 with 2 missing, elapsed time: 79.143\n",
      "Imputing row 9801/18213 with 1 missing, elapsed time: 79.189\n",
      "Imputing row 9901/18213 with 13 missing, elapsed time: 79.230\n",
      "Imputing row 10001/18213 with 0 missing, elapsed time: 79.268\n",
      "Imputing row 10101/18213 with 8 missing, elapsed time: 79.312\n",
      "Imputing row 10201/18213 with 2 missing, elapsed time: 79.357\n",
      "Imputing row 10301/18213 with 5 missing, elapsed time: 79.392\n",
      "Imputing row 10401/18213 with 22 missing, elapsed time: 79.431\n",
      "Imputing row 10501/18213 with 11 missing, elapsed time: 79.472\n",
      "Imputing row 10601/18213 with 3 missing, elapsed time: 79.540\n",
      "Imputing row 10701/18213 with 2 missing, elapsed time: 79.580\n",
      "Imputing row 10801/18213 with 5 missing, elapsed time: 79.634\n",
      "Imputing row 10901/18213 with 18 missing, elapsed time: 79.675\n",
      "Imputing row 11001/18213 with 1 missing, elapsed time: 79.725\n",
      "Imputing row 11101/18213 with 4 missing, elapsed time: 79.771\n",
      "Imputing row 11201/18213 with 2 missing, elapsed time: 79.803\n",
      "Imputing row 11301/18213 with 8 missing, elapsed time: 79.834\n",
      "Imputing row 11401/18213 with 4 missing, elapsed time: 79.869\n",
      "Imputing row 11501/18213 with 0 missing, elapsed time: 79.930\n",
      "Imputing row 11601/18213 with 1 missing, elapsed time: 79.978\n",
      "Imputing row 11701/18213 with 2 missing, elapsed time: 80.017\n",
      "Imputing row 11801/18213 with 7 missing, elapsed time: 80.059\n",
      "Imputing row 11901/18213 with 10 missing, elapsed time: 80.089\n",
      "Imputing row 12001/18213 with 7 missing, elapsed time: 80.130\n",
      "Imputing row 12101/18213 with 1 missing, elapsed time: 80.167\n",
      "Imputing row 12201/18213 with 3 missing, elapsed time: 80.217\n",
      "Imputing row 12301/18213 with 14 missing, elapsed time: 80.270\n",
      "Imputing row 12401/18213 with 5 missing, elapsed time: 80.320\n",
      "Imputing row 12501/18213 with 3 missing, elapsed time: 80.364\n",
      "Imputing row 12601/18213 with 2 missing, elapsed time: 80.403\n",
      "Imputing row 12701/18213 with 1 missing, elapsed time: 80.441\n",
      "Imputing row 12801/18213 with 4 missing, elapsed time: 80.488\n",
      "Imputing row 12901/18213 with 8 missing, elapsed time: 80.534\n",
      "Imputing row 13001/18213 with 3 missing, elapsed time: 80.569\n",
      "Imputing row 13101/18213 with 0 missing, elapsed time: 80.605\n",
      "Imputing row 13201/18213 with 13 missing, elapsed time: 80.658\n",
      "Imputing row 13301/18213 with 0 missing, elapsed time: 80.693\n",
      "Imputing row 13401/18213 with 1 missing, elapsed time: 80.725\n",
      "Imputing row 13501/18213 with 0 missing, elapsed time: 80.768\n",
      "Imputing row 13601/18213 with 11 missing, elapsed time: 80.813\n",
      "Imputing row 13701/18213 with 11 missing, elapsed time: 80.849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 13801/18213 with 1 missing, elapsed time: 80.878\n",
      "Imputing row 13901/18213 with 2 missing, elapsed time: 80.917\n",
      "Imputing row 14001/18213 with 4 missing, elapsed time: 80.951\n",
      "Imputing row 14101/18213 with 5 missing, elapsed time: 80.996\n",
      "Imputing row 14201/18213 with 1 missing, elapsed time: 81.044\n",
      "Imputing row 14301/18213 with 1 missing, elapsed time: 81.081\n",
      "Imputing row 14401/18213 with 0 missing, elapsed time: 81.119\n",
      "Imputing row 14501/18213 with 7 missing, elapsed time: 81.148\n",
      "Imputing row 14601/18213 with 0 missing, elapsed time: 81.188\n",
      "Imputing row 14701/18213 with 9 missing, elapsed time: 81.224\n",
      "Imputing row 14801/18213 with 1 missing, elapsed time: 81.254\n",
      "Imputing row 14901/18213 with 6 missing, elapsed time: 81.293\n",
      "Imputing row 15001/18213 with 5 missing, elapsed time: 81.331\n",
      "Imputing row 15101/18213 with 9 missing, elapsed time: 81.368\n",
      "Imputing row 15201/18213 with 5 missing, elapsed time: 81.421\n",
      "Imputing row 15301/18213 with 2 missing, elapsed time: 81.457\n",
      "Imputing row 15401/18213 with 1 missing, elapsed time: 81.493\n",
      "Imputing row 15501/18213 with 6 missing, elapsed time: 81.531\n",
      "Imputing row 15601/18213 with 1 missing, elapsed time: 81.563\n",
      "Imputing row 15701/18213 with 3 missing, elapsed time: 81.595\n",
      "Imputing row 15801/18213 with 7 missing, elapsed time: 81.643\n",
      "Imputing row 15901/18213 with 7 missing, elapsed time: 81.680\n",
      "Imputing row 16001/18213 with 0 missing, elapsed time: 81.715\n",
      "Imputing row 16101/18213 with 1 missing, elapsed time: 81.757\n",
      "Imputing row 16201/18213 with 0 missing, elapsed time: 81.801\n",
      "Imputing row 16301/18213 with 0 missing, elapsed time: 81.866\n",
      "Imputing row 16401/18213 with 3 missing, elapsed time: 81.919\n",
      "Imputing row 16501/18213 with 1 missing, elapsed time: 81.952\n",
      "Imputing row 16601/18213 with 1 missing, elapsed time: 81.989\n",
      "Imputing row 16701/18213 with 2 missing, elapsed time: 82.030\n",
      "Imputing row 16801/18213 with 4 missing, elapsed time: 82.059\n",
      "Imputing row 16901/18213 with 5 missing, elapsed time: 82.095\n",
      "Imputing row 17001/18213 with 2 missing, elapsed time: 82.127\n",
      "Imputing row 17101/18213 with 1 missing, elapsed time: 82.157\n",
      "Imputing row 17201/18213 with 0 missing, elapsed time: 82.200\n",
      "Imputing row 17301/18213 with 1 missing, elapsed time: 82.234\n",
      "Imputing row 17401/18213 with 11 missing, elapsed time: 82.274\n",
      "Imputing row 17501/18213 with 13 missing, elapsed time: 82.311\n",
      "Imputing row 17601/18213 with 7 missing, elapsed time: 82.348\n",
      "Imputing row 17701/18213 with 0 missing, elapsed time: 82.380\n",
      "Imputing row 17801/18213 with 5 missing, elapsed time: 82.430\n",
      "Imputing row 17901/18213 with 4 missing, elapsed time: 82.467\n",
      "Imputing row 18001/18213 with 1 missing, elapsed time: 82.501\n",
      "Imputing row 18101/18213 with 1 missing, elapsed time: 82.550\n",
      "Imputing row 18201/18213 with 1 missing, elapsed time: 82.584\n"
     ]
    }
   ],
   "source": [
    "knn_imputer = KNN(k=5)\n",
    "df_imputed = pd.DataFrame(knn_imputer.fit_transform(df[factors]), columns=factors, index=df[factors].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27368450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_imputed, df[['year', 'targeted',\"market_cap\",\"bic_level_2\",\"bic_level_3\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7db64b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 0s 318us/step\n",
      "82/82 [==============================] - 0s 308us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.495903  0.476470\n",
      "1        RF   0.622747  0.593968\n",
      "2       XGB   0.647005  0.610015\n",
      "3      LGBM   0.660908  0.632027\n",
      "4  CatBoost   0.652073  0.660517\n",
      "5        NN   0.922107  0.610481\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result1 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfc07a",
   "metadata": {},
   "source": [
    "### 1.2. oversampling with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352f181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Apply ADASYN oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9241c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798/798 [==============================] - 0s 298us/step\n",
      "82/82 [==============================] - 0s 305us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.571568  0.502407\n",
      "1        RF   0.999380  0.616202\n",
      "2       XGB   0.997711  0.634982\n",
      "3      LGBM   0.995835  0.617289\n",
      "4  CatBoost   0.995536  0.586793\n",
      "5        NN   0.998469  0.571224\n"
     ]
    }
   ],
   "source": [
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result2 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18d764",
   "metadata": {},
   "source": [
    "## 2.1 with column creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506b9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../database/2016-2022_semantic_imputation.csv')\n",
    "df['ev_ebitda'] = np.where((df['ev'] != 0) & (df['ebitda'] != 0), df['ev'] / df['ebitda'], np.nan)\n",
    "df['ev_ebit'] = np.where((df['ev'] != 0) & (df['ebit'] != 0), df['ev'] / df['ebit'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6775b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in non_ratio_variables:\n",
    "    \n",
    "    # 1. _percentile\n",
    "    percentile_col = col + '_percentile'\n",
    "    df[percentile_col] = df.groupby('year')[col].transform(lambda x: x.rank(pct=True) * 100)\n",
    "    df[percentile_col].fillna(50, inplace=True)\n",
    "    \n",
    "    # 2. _10bins_percentile\n",
    "    df['market_cap_bins'] = df.groupby('year')['market_cap'].transform(lambda x: pd.cut(x, bins=10))\n",
    "    percentile_10bins_col = col + '_10bins_percentile'\n",
    "    df[percentile_10bins_col] = df.groupby(['year', 'market_cap_bins'])[col].transform(lambda x: x.rank(pct=True) * 100)\n",
    "    df[percentile_10bins_col].fillna(50, inplace=True)\n",
    "    df.drop('market_cap_bins', axis=1, inplace=True)\n",
    "\n",
    "    # 3. _10bins_normalized\n",
    "    df['market_cap_bins'] = df.groupby('year')['market_cap'].transform(lambda x: pd.qcut(x, 10, labels=False, duplicates='drop'))\n",
    "    normalized_col = col + '_10bins_normalized'\n",
    "    df[normalized_col] = df.groupby(['year', 'market_cap_bins'])[col].transform(lambda x: (x - x.mean()) / x.std())\n",
    "    df[normalized_col].fillna(0, inplace=True)\n",
    "    df.drop('market_cap_bins', axis=1, inplace=True)\n",
    "    \n",
    "    # 4. _div_market_cap\n",
    "    div_market_cap_col = col + '_div_market_cap'\n",
    "    df[div_market_cap_col] = df[col] / df['market_cap']\n",
    "    \n",
    "    # 5. _div_log_market_cap\n",
    "    df['log_market_cap'] = np.log(df['market_cap'])\n",
    "    div_log_market_cap_col = col + '_div_log_market_cap'\n",
    "    df[div_log_market_cap_col] = df[col] / df['log_market_cap']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1626788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentile(group):\n",
    "    if len(group) < 10:\n",
    "        return pd.Series([None] * len(group), index=group.index, dtype=float)\n",
    "    return group.rank(pct=True) * 100\n",
    "\n",
    "def normalize(group):\n",
    "    if len(group) < 10:\n",
    "        return pd.Series([None] * len(group), index=group.index, dtype=float)\n",
    "    return (group - group.mean()) / group.std()\n",
    "\n",
    "for col in ratio_variables:\n",
    "    percentile_col = col + '_industry_peers_percentile'\n",
    "    df[percentile_col] = df.groupby(['year', 'bic_level_3'])[col].transform(compute_percentile)\n",
    "    mask = df[percentile_col].isna()\n",
    "    df.loc[mask, percentile_col] = df[mask].groupby(['year', 'bic_level_2'])[col].transform(compute_percentile)\n",
    "    df[percentile_col].fillna(50, inplace=True)\n",
    "    df[percentile_col] = df[percentile_col].astype(float)\n",
    "    normalized_col = col + '_industry_peers_normalized'\n",
    "    df[normalized_col] = df.groupby(['year', 'bic_level_3'])[col].transform(normalize)\n",
    "    mask = df[normalized_col].isna()\n",
    "    df.loc[mask, normalized_col] = df[mask].groupby(['year', 'bic_level_2'])[col].transform(normalize)\n",
    "    df[normalized_col].fillna(0, inplace=True)\n",
    "    df[normalized_col] = df[normalized_col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "900daeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = []\n",
    "for col in non_ratio_variables:\n",
    "    factors.extend([\n",
    "        col,\n",
    "        f'{col}_percentile',\n",
    "        f'{col}_10bins_percentile',\n",
    "        f'{col}_10bins_normalized',\n",
    "        f'{col}_div_market_cap',\n",
    "        f'{col}_div_log_market_cap'\n",
    "    ])\n",
    "\n",
    "for col in ratio_variables:\n",
    "    factors.extend([\n",
    "        col,\n",
    "        f'{col}_industry_peers_percentile',\n",
    "        f'{col}_industry_peers_normalized'\n",
    "    ])\n",
    "\n",
    "factors = factors + binary + technical_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b385d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/18213 with 7 missing, elapsed time: 155.718\n",
      "Imputing row 101/18213 with 6 missing, elapsed time: 155.837\n",
      "Imputing row 201/18213 with 7 missing, elapsed time: 155.908\n",
      "Imputing row 301/18213 with 8 missing, elapsed time: 155.970\n",
      "Imputing row 401/18213 with 1 missing, elapsed time: 156.032\n",
      "Imputing row 501/18213 with 0 missing, elapsed time: 156.116\n",
      "Imputing row 601/18213 with 1 missing, elapsed time: 156.188\n",
      "Imputing row 701/18213 with 11 missing, elapsed time: 156.253\n",
      "Imputing row 801/18213 with 7 missing, elapsed time: 156.317\n",
      "Imputing row 901/18213 with 1 missing, elapsed time: 156.380\n",
      "Imputing row 1001/18213 with 7 missing, elapsed time: 156.463\n",
      "Imputing row 1101/18213 with 6 missing, elapsed time: 156.545\n",
      "Imputing row 1201/18213 with 7 missing, elapsed time: 156.619\n",
      "Imputing row 1301/18213 with 16 missing, elapsed time: 156.693\n",
      "Imputing row 1401/18213 with 0 missing, elapsed time: 156.751\n",
      "Imputing row 1501/18213 with 30 missing, elapsed time: 156.809\n",
      "Imputing row 1601/18213 with 11 missing, elapsed time: 156.876\n",
      "Imputing row 1701/18213 with 1 missing, elapsed time: 156.933\n",
      "Imputing row 1801/18213 with 22 missing, elapsed time: 157.007\n",
      "Imputing row 1901/18213 with 1 missing, elapsed time: 157.082\n",
      "Imputing row 2001/18213 with 24 missing, elapsed time: 157.166\n",
      "Imputing row 2101/18213 with 16 missing, elapsed time: 157.236\n",
      "Imputing row 2201/18213 with 21 missing, elapsed time: 157.302\n",
      "Imputing row 2301/18213 with 1 missing, elapsed time: 157.377\n",
      "Imputing row 2401/18213 with 9 missing, elapsed time: 157.443\n",
      "Imputing row 2501/18213 with 6 missing, elapsed time: 157.515\n",
      "Imputing row 2601/18213 with 10 missing, elapsed time: 157.579\n",
      "Imputing row 2701/18213 with 12 missing, elapsed time: 157.644\n",
      "Imputing row 2801/18213 with 6 missing, elapsed time: 157.755\n",
      "Imputing row 2901/18213 with 1 missing, elapsed time: 157.821\n",
      "Imputing row 3001/18213 with 18 missing, elapsed time: 157.882\n",
      "Imputing row 3101/18213 with 11 missing, elapsed time: 157.946\n",
      "Imputing row 3201/18213 with 3 missing, elapsed time: 158.037\n",
      "Imputing row 3301/18213 with 8 missing, elapsed time: 158.103\n",
      "Imputing row 3401/18213 with 13 missing, elapsed time: 158.166\n",
      "Imputing row 3501/18213 with 21 missing, elapsed time: 158.232\n",
      "Imputing row 3601/18213 with 6 missing, elapsed time: 158.299\n",
      "Imputing row 3701/18213 with 8 missing, elapsed time: 158.393\n",
      "Imputing row 3801/18213 with 11 missing, elapsed time: 158.458\n",
      "Imputing row 3901/18213 with 10 missing, elapsed time: 158.536\n",
      "Imputing row 4001/18213 with 10 missing, elapsed time: 158.601\n",
      "Imputing row 4101/18213 with 15 missing, elapsed time: 158.670\n",
      "Imputing row 4201/18213 with 23 missing, elapsed time: 158.722\n",
      "Imputing row 4301/18213 with 0 missing, elapsed time: 158.782\n",
      "Imputing row 4401/18213 with 8 missing, elapsed time: 158.854\n",
      "Imputing row 4501/18213 with 18 missing, elapsed time: 158.926\n",
      "Imputing row 4601/18213 with 0 missing, elapsed time: 159.002\n",
      "Imputing row 4701/18213 with 7 missing, elapsed time: 159.078\n",
      "Imputing row 4801/18213 with 23 missing, elapsed time: 159.132\n",
      "Imputing row 4901/18213 with 25 missing, elapsed time: 159.200\n",
      "Imputing row 5001/18213 with 6 missing, elapsed time: 159.273\n",
      "Imputing row 5101/18213 with 2 missing, elapsed time: 159.337\n",
      "Imputing row 5201/18213 with 10 missing, elapsed time: 159.405\n",
      "Imputing row 5301/18213 with 23 missing, elapsed time: 159.476\n",
      "Imputing row 5401/18213 with 70 missing, elapsed time: 159.573\n",
      "Imputing row 5501/18213 with 1 missing, elapsed time: 159.679\n",
      "Imputing row 5601/18213 with 28 missing, elapsed time: 159.750\n",
      "Imputing row 5701/18213 with 1 missing, elapsed time: 159.808\n",
      "Imputing row 5801/18213 with 9 missing, elapsed time: 159.907\n",
      "Imputing row 5901/18213 with 1 missing, elapsed time: 159.988\n",
      "Imputing row 6001/18213 with 11 missing, elapsed time: 160.058\n",
      "Imputing row 6101/18213 with 6 missing, elapsed time: 160.126\n",
      "Imputing row 6201/18213 with 1 missing, elapsed time: 160.198\n",
      "Imputing row 6301/18213 with 14 missing, elapsed time: 160.285\n",
      "Imputing row 6401/18213 with 19 missing, elapsed time: 160.372\n",
      "Imputing row 6501/18213 with 18 missing, elapsed time: 160.438\n",
      "Imputing row 6601/18213 with 16 missing, elapsed time: 160.505\n",
      "Imputing row 6701/18213 with 0 missing, elapsed time: 160.567\n",
      "Imputing row 6801/18213 with 18 missing, elapsed time: 160.637\n",
      "Imputing row 6901/18213 with 0 missing, elapsed time: 160.700\n",
      "Imputing row 7001/18213 with 6 missing, elapsed time: 160.752\n",
      "Imputing row 7101/18213 with 17 missing, elapsed time: 160.826\n",
      "Imputing row 7201/18213 with 8 missing, elapsed time: 160.905\n",
      "Imputing row 7301/18213 with 30 missing, elapsed time: 160.977\n",
      "Imputing row 7401/18213 with 0 missing, elapsed time: 161.044\n",
      "Imputing row 7501/18213 with 0 missing, elapsed time: 161.108\n",
      "Imputing row 7601/18213 with 39 missing, elapsed time: 161.188\n",
      "Imputing row 7701/18213 with 2 missing, elapsed time: 161.253\n",
      "Imputing row 7801/18213 with 1 missing, elapsed time: 161.328\n",
      "Imputing row 7901/18213 with 23 missing, elapsed time: 161.392\n",
      "Imputing row 8001/18213 with 13 missing, elapsed time: 161.456\n",
      "Imputing row 8101/18213 with 10 missing, elapsed time: 161.587\n",
      "Imputing row 8201/18213 with 0 missing, elapsed time: 161.666\n",
      "Imputing row 8301/18213 with 5 missing, elapsed time: 161.728\n",
      "Imputing row 8401/18213 with 9 missing, elapsed time: 161.812\n",
      "Imputing row 8501/18213 with 4 missing, elapsed time: 161.901\n",
      "Imputing row 8601/18213 with 2 missing, elapsed time: 161.968\n",
      "Imputing row 8701/18213 with 0 missing, elapsed time: 162.049\n",
      "Imputing row 8801/18213 with 1 missing, elapsed time: 162.106\n",
      "Imputing row 8901/18213 with 10 missing, elapsed time: 162.183\n",
      "Imputing row 9001/18213 with 7 missing, elapsed time: 162.276\n",
      "Imputing row 9101/18213 with 13 missing, elapsed time: 162.344\n",
      "Imputing row 9201/18213 with 1 missing, elapsed time: 162.417\n",
      "Imputing row 9301/18213 with 11 missing, elapsed time: 162.468\n",
      "Imputing row 9401/18213 with 1 missing, elapsed time: 162.533\n",
      "Imputing row 9501/18213 with 0 missing, elapsed time: 162.591\n",
      "Imputing row 9601/18213 with 9 missing, elapsed time: 162.661\n",
      "Imputing row 9701/18213 with 6 missing, elapsed time: 162.731\n",
      "Imputing row 9801/18213 with 1 missing, elapsed time: 162.819\n",
      "Imputing row 9901/18213 with 29 missing, elapsed time: 162.894\n",
      "Imputing row 10001/18213 with 0 missing, elapsed time: 162.962\n",
      "Imputing row 10101/18213 with 12 missing, elapsed time: 163.037\n",
      "Imputing row 10201/18213 with 2 missing, elapsed time: 163.136\n",
      "Imputing row 10301/18213 with 9 missing, elapsed time: 163.212\n",
      "Imputing row 10401/18213 with 32 missing, elapsed time: 163.281\n",
      "Imputing row 10501/18213 with 23 missing, elapsed time: 163.342\n",
      "Imputing row 10601/18213 with 7 missing, elapsed time: 163.465\n",
      "Imputing row 10701/18213 with 6 missing, elapsed time: 163.539\n",
      "Imputing row 10801/18213 with 9 missing, elapsed time: 163.617\n",
      "Imputing row 10901/18213 with 22 missing, elapsed time: 163.687\n",
      "Imputing row 11001/18213 with 1 missing, elapsed time: 163.785\n",
      "Imputing row 11101/18213 with 8 missing, elapsed time: 163.866\n",
      "Imputing row 11201/18213 with 6 missing, elapsed time: 163.924\n",
      "Imputing row 11301/18213 with 18 missing, elapsed time: 163.979\n",
      "Imputing row 11401/18213 with 8 missing, elapsed time: 164.043\n",
      "Imputing row 11501/18213 with 0 missing, elapsed time: 164.130\n",
      "Imputing row 11601/18213 with 1 missing, elapsed time: 164.235\n",
      "Imputing row 11701/18213 with 2 missing, elapsed time: 164.305\n",
      "Imputing row 11801/18213 with 11 missing, elapsed time: 164.379\n",
      "Imputing row 11901/18213 with 20 missing, elapsed time: 164.432\n",
      "Imputing row 12001/18213 with 11 missing, elapsed time: 164.505\n",
      "Imputing row 12101/18213 with 1 missing, elapsed time: 164.569\n",
      "Imputing row 12201/18213 with 7 missing, elapsed time: 164.635\n",
      "Imputing row 12301/18213 with 30 missing, elapsed time: 164.708\n",
      "Imputing row 12401/18213 with 5 missing, elapsed time: 164.804\n",
      "Imputing row 12501/18213 with 7 missing, elapsed time: 164.884\n",
      "Imputing row 12601/18213 with 6 missing, elapsed time: 164.953\n",
      "Imputing row 12701/18213 with 1 missing, elapsed time: 165.025\n",
      "Imputing row 12801/18213 with 8 missing, elapsed time: 165.111\n",
      "Imputing row 12901/18213 with 18 missing, elapsed time: 165.174\n",
      "Imputing row 13001/18213 with 7 missing, elapsed time: 165.254\n",
      "Imputing row 13101/18213 with 0 missing, elapsed time: 165.316\n",
      "Imputing row 13201/18213 with 29 missing, elapsed time: 165.407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 13301/18213 with 0 missing, elapsed time: 165.463\n",
      "Imputing row 13401/18213 with 1 missing, elapsed time: 165.515\n",
      "Imputing row 13501/18213 with 0 missing, elapsed time: 165.584\n",
      "Imputing row 13601/18213 with 23 missing, elapsed time: 165.662\n",
      "Imputing row 13701/18213 with 23 missing, elapsed time: 165.722\n",
      "Imputing row 13801/18213 with 1 missing, elapsed time: 165.770\n",
      "Imputing row 13901/18213 with 2 missing, elapsed time: 165.831\n",
      "Imputing row 14001/18213 with 12 missing, elapsed time: 165.885\n",
      "Imputing row 14101/18213 with 11 missing, elapsed time: 165.977\n",
      "Imputing row 14201/18213 with 1 missing, elapsed time: 166.041\n",
      "Imputing row 14301/18213 with 1 missing, elapsed time: 166.102\n",
      "Imputing row 14401/18213 with 0 missing, elapsed time: 166.162\n",
      "Imputing row 14501/18213 with 17 missing, elapsed time: 166.207\n",
      "Imputing row 14601/18213 with 0 missing, elapsed time: 166.275\n",
      "Imputing row 14701/18213 with 15 missing, elapsed time: 166.318\n",
      "Imputing row 14801/18213 with 1 missing, elapsed time: 166.370\n",
      "Imputing row 14901/18213 with 16 missing, elapsed time: 166.436\n",
      "Imputing row 15001/18213 with 5 missing, elapsed time: 166.497\n",
      "Imputing row 15101/18213 with 13 missing, elapsed time: 166.570\n",
      "Imputing row 15201/18213 with 11 missing, elapsed time: 166.622\n",
      "Imputing row 15301/18213 with 6 missing, elapsed time: 166.680\n",
      "Imputing row 15401/18213 with 1 missing, elapsed time: 166.741\n",
      "Imputing row 15501/18213 with 12 missing, elapsed time: 166.800\n",
      "Imputing row 15601/18213 with 1 missing, elapsed time: 166.851\n",
      "Imputing row 15701/18213 with 3 missing, elapsed time: 166.899\n",
      "Imputing row 15801/18213 with 13 missing, elapsed time: 166.964\n",
      "Imputing row 15901/18213 with 11 missing, elapsed time: 167.014\n",
      "Imputing row 16001/18213 with 0 missing, elapsed time: 167.071\n",
      "Imputing row 16101/18213 with 1 missing, elapsed time: 167.133\n",
      "Imputing row 16201/18213 with 0 missing, elapsed time: 167.210\n",
      "Imputing row 16301/18213 with 0 missing, elapsed time: 167.274\n",
      "Imputing row 16401/18213 with 3 missing, elapsed time: 167.321\n",
      "Imputing row 16501/18213 with 1 missing, elapsed time: 167.369\n",
      "Imputing row 16601/18213 with 1 missing, elapsed time: 167.431\n",
      "Imputing row 16701/18213 with 2 missing, elapsed time: 167.501\n",
      "Imputing row 16801/18213 with 4 missing, elapsed time: 167.563\n",
      "Imputing row 16901/18213 with 5 missing, elapsed time: 167.638\n",
      "Imputing row 17001/18213 with 2 missing, elapsed time: 167.696\n",
      "Imputing row 17101/18213 with 1 missing, elapsed time: 167.742\n",
      "Imputing row 17201/18213 with 0 missing, elapsed time: 167.787\n",
      "Imputing row 17301/18213 with 1 missing, elapsed time: 167.839\n",
      "Imputing row 17401/18213 with 23 missing, elapsed time: 167.917\n",
      "Imputing row 17501/18213 with 29 missing, elapsed time: 167.979\n",
      "Imputing row 17601/18213 with 15 missing, elapsed time: 168.033\n",
      "Imputing row 17701/18213 with 0 missing, elapsed time: 168.090\n",
      "Imputing row 17801/18213 with 13 missing, elapsed time: 168.155\n",
      "Imputing row 17901/18213 with 10 missing, elapsed time: 168.208\n",
      "Imputing row 18001/18213 with 1 missing, elapsed time: 168.259\n",
      "Imputing row 18101/18213 with 1 missing, elapsed time: 168.319\n",
      "Imputing row 18201/18213 with 1 missing, elapsed time: 168.370\n"
     ]
    }
   ],
   "source": [
    "knn_imputer = KNN(k=5)\n",
    "df_imputed = pd.DataFrame(knn_imputer.fit_transform(df[factors]), columns=factors, index=df[factors].index)\n",
    "df = pd.concat([df_imputed, df[['year', 'targeted',\"market_cap\",\"bic_level_2\",\"bic_level_3\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63879812",
   "metadata": {},
   "source": [
    "### 2.1. raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8466b0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 0s 325us/step\n",
      "82/82 [==============================] - 0s 332us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.524704  0.502557\n",
      "1        RF   0.595857  0.603905\n",
      "2       XGB   0.652837  0.659762\n",
      "3      LGBM   0.664124  0.642298\n",
      "4  CatBoost   0.648389  0.647255\n",
      "5        NN   0.943861  0.616496\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result3 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c2684",
   "metadata": {},
   "source": [
    "### 2.2. oversampling the ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a233d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798/798 [==============================] - 0s 318us/step\n",
      "82/82 [==============================] - 0s 338us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.593854  0.540768\n",
      "1        RF   0.999734  0.636969\n",
      "2       XGB   0.998261  0.637804\n",
      "3      LGBM   0.996751  0.643373\n",
      "4  CatBoost   0.995105  0.606466\n",
      "5        NN   0.999999  0.624232\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Apply ADASYN oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result4 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
