{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2acb5f",
   "metadata": {},
   "source": [
    "# Structure\n",
    "\n",
    "median imputation\n",
    "- 1. no column creation\n",
    "    - 1.1. raw data\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "    - 1.2. ADASYN imputation\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "- 2. yes column creation\n",
    "    - 2.1. raw data\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "    - 2.2. ADASYN imputation\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a391119",
   "metadata": {},
   "source": [
    "## Load the data and the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46fdda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce798c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../database/2016-2022_semantic_imputation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57082e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = ['unequal_voting','classified_board_system','poison_pill','operating_margin_below_3y_average']\n",
    "non_ratio_variables = [\n",
    "    \"capex\",\n",
    "    \"net_capex\",\n",
    "    \"short_term_wc\",\n",
    "    \"long_term_wc\",\n",
    "    \"modified_wc\",\n",
    "    \"ebitda\",\n",
    "    \"ebit\",\n",
    "    \"net_income\",\n",
    "    \"net_debt\",\n",
    "    \"ev\",\n",
    "    \"repurchase\",\n",
    "    \"board_size\",\n",
    "    \"net_repurchase\",\n",
    "    \"total_compensation_to_executives\",\n",
    "    \"total_compensation_to_board_members\",\n",
    "    \"dividend_to_common\",\n",
    "    \"dividend_to_preferred\"\n",
    "]\n",
    "\n",
    "df['ev_ebitda'] = np.where((df['ev'] != 0) & (df['ebitda'] != 0), df['ev'] / df['ebitda'], np.nan)\n",
    "df['ev_ebit'] = np.where((df['ev'] != 0) & (df['ebit'] != 0), df['ev'] / df['ebit'], np.nan)\n",
    "\n",
    "ratio_variables = [\n",
    "    \"ebitda_margin\",\n",
    "    \"operating_margin\",\n",
    "    \"sales_to_total_assets\",\n",
    "    \"roe\",\n",
    "    \"normalized_roe\",\n",
    "    \"operating_roe\",\n",
    "    \"operating_roic\",\n",
    "    \"eps_adjusted_diluted\",\n",
    "    \"ev_to_sales\",\n",
    "    \"tobin_q_ratio\",\n",
    "    \"pb_ratio\",\n",
    "    \"pe_ratio\",\n",
    "    \"fcf_to_equity\",\n",
    "    \"sales_growth_rate\",\n",
    "    \"dividend_per_share\",\n",
    "    \"dividend_payout_ratio\",\n",
    "    \"asset_to_equity\",\n",
    "    \"cash_conversion_cycle\",\n",
    "    \"ev_ebitda\",\n",
    "    \"ev_ebit\",\n",
    "]\n",
    "\n",
    "technical_variables = [\n",
    "    \"free_float_percentage\",\n",
    "    \"rsi_14d\",\n",
    "    \"rsi_30d\",\n",
    "    \"volatility_30d\",\n",
    "    \"volatility_90d\",\n",
    "    \"volatility_180d\",\n",
    "    \"volume_30d_average_to_outstanding\",\n",
    "    \"insider_shares_percentage\",\n",
    "    \"institution_ownership_percentage\",\n",
    "    \"ceo_tenure\",\n",
    "    \"total_return_5y\",\n",
    "    \"total_return_4y\",\n",
    "    \"total_return_3y\",\n",
    "    \"total_return_2y\",\n",
    "    \"total_return_1y\",\n",
    "    \"total_return_6m\",\n",
    "    \"total_return_3m\",\n",
    "    \"employee_growth_rate\",\n",
    "    \"fcf_yield\"\n",
    "]\n",
    "\n",
    "supportive = [\"bic_level_2\",\"bic_level_3\",\"market_cap\"]\n",
    "factors = binary + non_ratio_variables + ratio_variables + technical_variables\n",
    "\n",
    "df[\"bic_level_2\"] = df[\"bic_level_2\"].astype('category')\n",
    "df[\"bic_level_3\"] = df[\"bic_level_3\"].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897f954",
   "metadata": {},
   "source": [
    "## 1. No column creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1007d1",
   "metadata": {},
   "source": [
    "### 1.1. raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66a501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians_by_year = df.groupby('year')[factors].transform('median')\n",
    "df[factors] = df[factors].fillna(medians_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7db64b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 0s 302us/step\n",
      "82/82 [==============================] - 0s 306us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.473548  0.470346\n",
      "1        RF   0.622123  0.593709\n",
      "2       XGB   0.650936  0.648598\n",
      "3      LGBM   0.656551  0.619298\n",
      "4  CatBoost   0.649825  0.623635\n",
      "5        NN   0.920319  0.595442\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame and factors is your list of columns\n",
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result1 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfc07a",
   "metadata": {},
   "source": [
    "### 1.2. oversampling with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352f181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Apply ADASYN oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Cross-validation setup\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9241c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 0s 299us/step\n",
      "82/82 [==============================] - 0s 305us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.473548  0.470346\n",
      "1        RF   0.637964  0.613362\n",
      "2       XGB   0.650936  0.648598\n",
      "3      LGBM   0.656551  0.619298\n",
      "4  CatBoost   0.649825  0.623635\n",
      "5        NN   0.912614  0.608892\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame and factors is your list of columns\n",
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result2 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18d764",
   "metadata": {},
   "source": [
    "## 2.1 with column creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8694e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../database/2016-2022_semantic_imputation.csv')\n",
    "df['ev_ebitda'] = np.where((df['ev'] != 0) & (df['ebitda'] != 0), df['ev'] / df['ebitda'], np.nan)\n",
    "df['ev_ebit'] = np.where((df['ev'] != 0) & (df['ebit'] != 0), df['ev'] / df['ebit'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6775b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in non_ratio_variables:\n",
    "    \n",
    "    # 1. _percentile\n",
    "    percentile_col = col + '_percentile'\n",
    "    df[percentile_col] = df.groupby('year')[col].transform(lambda x: x.rank(pct=True) * 100)\n",
    "    df[percentile_col].fillna(50, inplace=True)\n",
    "    \n",
    "    # 2. _10bins_percentile\n",
    "    df['market_cap_bins'] = df.groupby('year')['market_cap'].transform(lambda x: pd.cut(x, bins=10))\n",
    "    percentile_10bins_col = col + '_10bins_percentile'\n",
    "    df[percentile_10bins_col] = df.groupby(['year', 'market_cap_bins'])[col].transform(lambda x: x.rank(pct=True) * 100)\n",
    "    df[percentile_10bins_col].fillna(50, inplace=True)\n",
    "    df.drop('market_cap_bins', axis=1, inplace=True)\n",
    "\n",
    "    # 3. _10bins_normalized\n",
    "    df['market_cap_bins'] = df.groupby('year')['market_cap'].transform(lambda x: pd.qcut(x, 10, labels=False, duplicates='drop'))\n",
    "    normalized_col = col + '_10bins_normalized'\n",
    "    df[normalized_col] = df.groupby(['year', 'market_cap_bins'])[col].transform(lambda x: (x - x.mean()) / x.std())\n",
    "    df[normalized_col].fillna(0, inplace=True)\n",
    "    df.drop('market_cap_bins', axis=1, inplace=True)\n",
    "    \n",
    "    # 4. _div_market_cap\n",
    "    div_market_cap_col = col + '_div_market_cap'\n",
    "    df[div_market_cap_col] = df[col] / df['market_cap']\n",
    "    \n",
    "    # 5. _div_log_market_cap\n",
    "    df['log_market_cap'] = np.log(df['market_cap'])\n",
    "    div_log_market_cap_col = col + '_div_log_market_cap'\n",
    "    df[div_log_market_cap_col] = df[col] / df['log_market_cap']\n",
    "    \n",
    "    for new_col in [div_market_cap_col, div_log_market_cap_col]:\n",
    "        median_values = df.groupby('year')[new_col].transform('median')\n",
    "        df[new_col].fillna(median_values, inplace=True)\n",
    "    df.drop('log_market_cap', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1626788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentile(group):\n",
    "    if len(group) < 10:\n",
    "        return pd.Series([None] * len(group), index=group.index, dtype=float)\n",
    "    return group.rank(pct=True) * 100\n",
    "\n",
    "def normalize(group):\n",
    "    if len(group) < 10:\n",
    "        return pd.Series([None] * len(group), index=group.index, dtype=float)\n",
    "    return (group - group.mean()) / group.std()\n",
    "\n",
    "for col in ratio_variables:\n",
    "    percentile_col = col + '_industry_peers_percentile'\n",
    "    df[percentile_col] = df.groupby(['year', 'bic_level_3'])[col].transform(compute_percentile)\n",
    "    mask = df[percentile_col].isna()\n",
    "    df.loc[mask, percentile_col] = df[mask].groupby(['year', 'bic_level_2'])[col].transform(compute_percentile)\n",
    "    df[percentile_col].fillna(50, inplace=True)\n",
    "    df[percentile_col] = df[percentile_col].astype(float)\n",
    "    normalized_col = col + '_industry_peers_normalized'\n",
    "    df[normalized_col] = df.groupby(['year', 'bic_level_3'])[col].transform(normalize)\n",
    "    mask = df[normalized_col].isna()\n",
    "    df.loc[mask, normalized_col] = df[mask].groupby(['year', 'bic_level_2'])[col].transform(normalize)\n",
    "    df[normalized_col].fillna(0, inplace=True)\n",
    "    df[normalized_col] = df[normalized_col].astype(float)\n",
    "    df[col].fillna(df.groupby('year')[col].transform('median'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "900daeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = []\n",
    "for col in non_ratio_variables:\n",
    "    factors.extend([\n",
    "        col,\n",
    "        f'{col}_percentile',\n",
    "        f'{col}_10bins_percentile',\n",
    "        f'{col}_10bins_normalized',\n",
    "        f'{col}_div_market_cap',\n",
    "        f'{col}_div_log_market_cap'\n",
    "    ])\n",
    "\n",
    "for col in ratio_variables:\n",
    "    factors.extend([\n",
    "        col,\n",
    "        f'{col}_industry_peers_percentile',\n",
    "        f'{col}_industry_peers_normalized'\n",
    "    ])\n",
    "\n",
    "factors = factors + binary + technical_variables\n",
    "medians_by_year = df.groupby('year')[factors].transform('median')\n",
    "df[factors] = df[factors].fillna(medians_by_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63879812",
   "metadata": {},
   "source": [
    "### 2.1. raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1c7bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Cross-validation setup\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8466b0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 0s 337us/step\n",
      "82/82 [==============================] - 0s 346us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.515446  0.500475\n",
      "1        RF   0.617702  0.589390\n",
      "2       XGB   0.647668  0.645038\n",
      "3      LGBM   0.664130  0.643706\n",
      "4  CatBoost   0.634286  0.664675\n",
      "5        NN   0.940113  0.610257\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame and factors is your list of columns\n",
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result3 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c2684",
   "metadata": {},
   "source": [
    "### 2.2. oversampling the ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40fcde23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Apply ADASYN oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Cross-validation setup\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a233d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/minwukim/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 0s 346us/step\n",
      "82/82 [==============================] - 0s 334us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.515446  0.500475\n",
      "1        RF   0.617866  0.576469\n",
      "2       XGB   0.647668  0.645038\n",
      "3      LGBM   0.664130  0.643706\n",
      "4  CatBoost   0.634286  0.664675\n",
      "5        NN   0.940867  0.630198\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame and factors is your list of columns\n",
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result4 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
