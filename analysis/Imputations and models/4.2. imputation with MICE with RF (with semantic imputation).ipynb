{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2acb5f",
   "metadata": {},
   "source": [
    "# Structure\n",
    "\n",
    "Imputation with MICE with RF (MissForest)\n",
    "- 1. no column creation\n",
    "    - 1.1. raw data\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "    - 1.2. ADASYN imputation\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "- 2. yes column creation\n",
    "    - 2.1. raw data\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network\n",
    "    - 2.2. ADASYN imputation\n",
    "        - LR, RF, XGboost, LightGBM, Catboost, Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a391119",
   "metadata": {},
   "source": [
    "## Load the data and the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46fdda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.neighbors._base \n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from missforest.missforest import MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce798c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../database/2016-2022_semantic_imputation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57082e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = ['unequal_voting','classified_board_system','poison_pill','operating_margin_below_3y_average']\n",
    "non_ratio_variables = [\n",
    "    \"capex\",\n",
    "    \"net_capex\",\n",
    "    \"short_term_wc\",\n",
    "    \"long_term_wc\",\n",
    "    \"modified_wc\",\n",
    "    \"ebitda\",\n",
    "    \"ebit\",\n",
    "    \"net_income\",\n",
    "    \"net_debt\",\n",
    "    \"ev\",\n",
    "    \"repurchase\",\n",
    "    \"board_size\",\n",
    "    \"net_repurchase\",\n",
    "    \"total_compensation_to_executives\",\n",
    "    \"total_compensation_to_board_members\",\n",
    "    \"dividend_to_common\",\n",
    "    \"dividend_to_preferred\"\n",
    "]\n",
    "\n",
    "df['ev_ebitda'] = np.where((df['ev'] != 0) & (df['ebitda'] != 0), df['ev'] / df['ebitda'], np.nan)\n",
    "df['ev_ebit'] = np.where((df['ev'] != 0) & (df['ebit'] != 0), df['ev'] / df['ebit'], np.nan)\n",
    "\n",
    "ratio_variables = [\n",
    "    \"ebitda_margin\",\n",
    "    \"operating_margin\",\n",
    "    \"sales_to_total_assets\",\n",
    "    \"roe\",\n",
    "    \"normalized_roe\",\n",
    "    \"operating_roe\",\n",
    "    \"operating_roic\",\n",
    "    \"eps_adjusted_diluted\",\n",
    "    \"ev_to_sales\",\n",
    "    \"tobin_q_ratio\",\n",
    "    \"pb_ratio\",\n",
    "    \"pe_ratio\",\n",
    "    \"fcf_to_equity\",\n",
    "    \"sales_growth_rate\",\n",
    "    \"dividend_per_share\",\n",
    "    \"dividend_payout_ratio\",\n",
    "    \"asset_to_equity\",\n",
    "    \"cash_conversion_cycle\",\n",
    "    \"ev_ebitda\",\n",
    "    \"ev_ebit\",\n",
    "]\n",
    "\n",
    "technical_variables = [\n",
    "    \"free_float_percentage\",\n",
    "    \"rsi_14d\",\n",
    "    \"rsi_30d\",\n",
    "    \"volatility_30d\",\n",
    "    \"volatility_90d\",\n",
    "    \"volatility_180d\",\n",
    "    \"volume_30d_average_to_outstanding\",\n",
    "    \"insider_shares_percentage\",\n",
    "    \"institution_ownership_percentage\",\n",
    "    \"ceo_tenure\",\n",
    "    \"total_return_5y\",\n",
    "    \"total_return_4y\",\n",
    "    \"total_return_3y\",\n",
    "    \"total_return_2y\",\n",
    "    \"total_return_1y\",\n",
    "    \"total_return_6m\",\n",
    "    \"total_return_3m\",\n",
    "    \"employee_growth_rate\",\n",
    "    \"fcf_yield\"\n",
    "]\n",
    "\n",
    "supportive = [\"bic_level_2\",\"bic_level_3\",\"market_cap\"]\n",
    "factors = binary + non_ratio_variables + ratio_variables + technical_variables\n",
    "\n",
    "df[\"bic_level_2\"] = df[\"bic_level_2\"].astype('category')\n",
    "df[\"bic_level_3\"] = df[\"bic_level_3\"].astype('category')\n",
    "\n",
    "# factors.append(\"targeted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897f954",
   "metadata": {},
   "source": [
    "## 1. No column creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1007d1",
   "metadata": {},
   "source": [
    "### 1.1. raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27368450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe and select only the factors you want to impute\n",
    "df_amp = df[factors].copy()\n",
    "\n",
    "# Use MissForest from missingpy for imputation\n",
    "imputer = MissForest()\n",
    "df_imputed = imputer.fit_transform(df_amp)\n",
    "\n",
    "# Convert the result back to a dataframe\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=factors)\n",
    "\n",
    "# Now merge back the imputed data with the original dataframe\n",
    "df = pd.concat([df_imputed, df[['year', 'targeted', \"market_cap\", \"bic_level_2\", \"bic_level_3\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7db64b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 0s 382us/step\n",
      "82/82 [==============================] - 0s 397us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.494861  0.483239\n",
      "1        RF   0.619853  0.602037\n",
      "2       XGB   0.635359  0.615742\n",
      "3      LGBM   0.648180  0.638470\n",
      "4  CatBoost   0.648393  0.626637\n",
      "5        NN   0.928767  0.626257\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result1 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfc07a",
   "metadata": {},
   "source": [
    "### 1.2. oversampling with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352f181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Apply ADASYN oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9241c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 297us/step\n",
      "82/82 [==============================] - 0s 307us/step\n",
      "      Model  Train AUC  Test AUC\n",
      "0        LR   0.543917  0.484922\n",
      "1        RF   0.999483  0.641493\n",
      "2       XGB   0.997942  0.626848\n",
      "3      LGBM   0.995800  0.612479\n",
      "4  CatBoost   0.995205  0.602029\n",
      "5        NN   0.998597  0.582144\n"
     ]
    }
   ],
   "source": [
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result2 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18d764",
   "metadata": {},
   "source": [
    "## 2.1 with column creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "506b9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../database/2016-2022_semantic_imputation.csv')\n",
    "df['ev_ebitda'] = np.where((df['ev'] != 0) & (df['ebitda'] != 0), df['ev'] / df['ebitda'], np.nan)\n",
    "df['ev_ebit'] = np.where((df['ev'] != 0) & (df['ebit'] != 0), df['ev'] / df['ebit'], np.nan)\n",
    "df[\"bic_level_2\"] = df[\"bic_level_2\"].astype('category')\n",
    "df[\"bic_level_3\"] = df[\"bic_level_3\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6775b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in non_ratio_variables:\n",
    "    \n",
    "    # 1. _percentile\n",
    "    percentile_col = col + '_percentile'\n",
    "    df[percentile_col] = df.groupby('year')[col].transform(lambda x: x.rank(pct=True) * 100)\n",
    "    df[percentile_col].fillna(50, inplace=True)\n",
    "    \n",
    "    # 2. _10bins_percentile\n",
    "    df['market_cap_bins'] = df.groupby('year')['market_cap'].transform(lambda x: pd.cut(x, bins=10))\n",
    "    percentile_10bins_col = col + '_10bins_percentile'\n",
    "    df[percentile_10bins_col] = df.groupby(['year', 'market_cap_bins'])[col].transform(lambda x: x.rank(pct=True) * 100)\n",
    "    df[percentile_10bins_col].fillna(50, inplace=True)\n",
    "    df.drop('market_cap_bins', axis=1, inplace=True)\n",
    "\n",
    "    # 3. _10bins_normalized\n",
    "    df['market_cap_bins'] = df.groupby('year')['market_cap'].transform(lambda x: pd.qcut(x, 10, labels=False, duplicates='drop'))\n",
    "    normalized_col = col + '_10bins_normalized'\n",
    "    df[normalized_col] = df.groupby(['year', 'market_cap_bins'])[col].transform(lambda x: (x - x.mean()) / x.std())\n",
    "    df[normalized_col].fillna(0, inplace=True)\n",
    "    df.drop('market_cap_bins', axis=1, inplace=True)\n",
    "    \n",
    "    # 4. _div_market_cap\n",
    "    div_market_cap_col = col + '_div_market_cap'\n",
    "    df[div_market_cap_col] = df[col] / df['market_cap']\n",
    "    \n",
    "    # 5. _div_log_market_cap\n",
    "    df['log_market_cap'] = np.log(df['market_cap'])\n",
    "    div_log_market_cap_col = col + '_div_log_market_cap'\n",
    "    df[div_log_market_cap_col] = df[col] / df['log_market_cap']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1626788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentile(group):\n",
    "    if len(group) < 10:\n",
    "        return pd.Series([None] * len(group), index=group.index, dtype=float)\n",
    "    return group.rank(pct=True) * 100\n",
    "\n",
    "def normalize(group):\n",
    "    if len(group) < 10:\n",
    "        return pd.Series([None] * len(group), index=group.index, dtype=float)\n",
    "    return (group - group.mean()) / group.std()\n",
    "\n",
    "for col in ratio_variables:\n",
    "    percentile_col = col + '_industry_peers_percentile'\n",
    "    df[percentile_col] = df.groupby(['year', 'bic_level_3'])[col].transform(compute_percentile)\n",
    "    mask = df[percentile_col].isna()\n",
    "    df.loc[mask, percentile_col] = df[mask].groupby(['year', 'bic_level_2'])[col].transform(compute_percentile)\n",
    "    df[percentile_col].fillna(50, inplace=True)\n",
    "    df[percentile_col] = df[percentile_col].astype(float)\n",
    "    normalized_col = col + '_industry_peers_normalized'\n",
    "    df[normalized_col] = df.groupby(['year', 'bic_level_3'])[col].transform(normalize)\n",
    "    mask = df[normalized_col].isna()\n",
    "    df.loc[mask, normalized_col] = df[mask].groupby(['year', 'bic_level_2'])[col].transform(normalize)\n",
    "    df[normalized_col].fillna(0, inplace=True)\n",
    "    df[normalized_col] = df[normalized_col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "900daeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = []\n",
    "for col in non_ratio_variables:\n",
    "    factors.extend([\n",
    "        col,\n",
    "        f'{col}_percentile',\n",
    "        f'{col}_10bins_percentile',\n",
    "        f'{col}_10bins_normalized',\n",
    "        f'{col}_div_market_cap',\n",
    "        f'{col}_div_log_market_cap'\n",
    "    ])\n",
    "\n",
    "for col in ratio_variables:\n",
    "    factors.extend([\n",
    "        col,\n",
    "        f'{col}_industry_peers_percentile',\n",
    "        f'{col}_industry_peers_normalized'\n",
    "    ])\n",
    "\n",
    "factors = factors + binary + technical_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "506ca6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18213, 185)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amp2 = df[factors].copy()\n",
    "df_amp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b385d21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input data must be 2 dimensional and non empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m df_amp2 \u001b[38;5;241m=\u001b[39m df[factors]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Use MissForest from missingpy for imputation\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df_imputed \u001b[38;5;241m=\u001b[39m imputer\u001b[38;5;241m.\u001b[39mfit_transform(df_amp2)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the result back to a dataframe\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df_imputed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df_imputed, columns\u001b[38;5;241m=\u001b[39mfactors)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/missforest/missforest.py:332\u001b[0m, in \u001b[0;36mMissForest.fit_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m X_missing \u001b[38;5;241m=\u001b[39m X_imp\u001b[38;5;241m.\u001b[39mloc[miss_index]\n\u001b[1;32m    331\u001b[0m X_missing \u001b[38;5;241m=\u001b[39m X_missing\u001b[38;5;241m.\u001b[39mdrop(c, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 332\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict(X_missing)\n\u001b[1;32m    333\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(y_pred)\n\u001b[1;32m    334\u001b[0m y_pred\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m miss_row[c]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/sklearn.py:803\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m!=\u001b[39m n_features:\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features of the model must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    801\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch the input. Model n_features_ is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    802\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput n_features is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mpredict(X, raw_score\u001b[38;5;241m=\u001b[39mraw_score, start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration, num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[1;32m    804\u001b[0m                              pred_leaf\u001b[38;5;241m=\u001b[39mpred_leaf, pred_contrib\u001b[38;5;241m=\u001b[39mpred_contrib, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/basic.py:3538\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[1;32m   3536\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3537\u001b[0m         num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 3538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictor\u001b[38;5;241m.\u001b[39mpredict(data, start_iteration, num_iteration,\n\u001b[1;32m   3539\u001b[0m                          raw_score, pred_leaf, pred_contrib,\n\u001b[1;32m   3540\u001b[0m                          data_has_header, is_reshape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/basic.py:820\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Dataset):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use Dataset instance for prediction, please use raw data instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 820\u001b[0m data \u001b[38;5;241m=\u001b[39m _data_from_pandas(data, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    821\u001b[0m predict_type \u001b[38;5;241m=\u001b[39m C_API_PREDICT_NORMAL\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_score:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/basic.py:566\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[0;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 566\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput data must be 2 dimensional and non empty.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m feature_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Input data must be 2 dimensional and non empty."
     ]
    }
   ],
   "source": [
    "# Create a copy of the dataframe and select only the factors you want to impute\n",
    "df_amp2 = df[factors].copy()\n",
    "\n",
    "# Use MissForest from missingpy for imputation\n",
    "df_imputed = imputer.fit_transform(df_amp2)\n",
    "\n",
    "# Convert the result back to a dataframe\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=factors)\n",
    "\n",
    "# Now merge back the imputed data with the original dataframe\n",
    "df = pd.concat([df_imputed, df[['year', 'targeted', \"market_cap\", \"bic_level_2\", \"bic_level_3\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63879812",
   "metadata": {},
   "source": [
    "### 2.1. raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result3 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c2684",
   "metadata": {},
   "source": [
    "### 2.2. oversampling the ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a233d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[df['year'].isin([2016, 2017, 2018, 2019, 2020])]\n",
    "test_data = df[df['year'] == 2021]\n",
    "\n",
    "X_train = train_data[factors]\n",
    "y_train = train_data['targeted']\n",
    "\n",
    "X_test = test_data[factors]\n",
    "y_test = test_data['targeted']\n",
    "\n",
    "# Apply ADASYN oversampling\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def create_nn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Models (without neural network for now)\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(max_iter=10000),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGB\": xgb.XGBClassifier(),\n",
    "    \"LGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": cb.CatBoostClassifier(verbose=0, iterations=100)\n",
    "}\n",
    "\n",
    "# Setup for Cross Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "# Storage for AUC scores\n",
    "train_aucs = {}\n",
    "test_aucs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_aucs[model_name] = roc_auc_score(y_train, y_pred_train)\n",
    "    test_aucs[model_name] = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "# Neural Network\n",
    "nn_model = create_nn()\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "y_pred_train_nn = nn_model.predict(X_train_scaled)\n",
    "y_pred_test_nn = nn_model.predict(X_test_scaled)\n",
    "train_aucs[\"NN\"] = roc_auc_score(y_train, y_pred_train_nn)\n",
    "test_aucs[\"NN\"] = roc_auc_score(y_test, y_pred_test_nn)\n",
    "\n",
    "# Compile Results\n",
    "result4 = pd.DataFrame({\n",
    "    'Model': list(train_aucs.keys()),\n",
    "    'Train AUC': list(train_aucs.values()),\n",
    "    'Test AUC': list(test_aucs.values())\n",
    "})\n",
    "print(result4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
